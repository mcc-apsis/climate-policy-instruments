{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class args():\n",
    "    pass\n",
    "args.model_name = \"climatebert/distilroberta-base-climate-f\"\n",
    "args.y_prefix = \"10\"\n",
    "y_prefix = args.y_prefix\n",
    "min_samples = 10\n",
    "inclusion_var=\"INCLUDE\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_feather('../data/labels.feather')\n",
    "df = (df\n",
    "      .sort_values('id')\n",
    "      .sample(frac=1, random_state=1)\n",
    "      .reset_index(drop=True)\n",
    ")\n",
    "df['seen']=1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['10 - 3. Quantitative', '10 - 4. Qualitative']"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "targets = [x for x in df.columns if re.match(f\"^{y_prefix}\",x)]\n",
    "targets = [x for x in targets if df[x].sum()>min_samples]\n",
    "df = df.replace(-1,np.NaN)\n",
    "df = df.replace(2, 1)\n",
    "df = df.dropna(subset=targets).reset_index(drop=True)\n",
    "if y_prefix in inclusion_var:\n",
    "    nonrandom_index = df[df[\"representative_sample\"]==0].index\n",
    "    df['labels'] = df[targets[0]]\n",
    "else:\n",
    "    df = df[df[inclusion_var]==1].reset_index(drop=True)\n",
    "    nonrandom_index = df[df[\"representative_relevant\"]==0].index\n",
    "    df['labels'] = list(df[targets].values.astype(int))\n",
    "\n",
    "targets\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['nacsos_id', 'title', 'abstract', '0 - relevant',\n",
       "       '10 - 3. Quantitative', '10 - 4. Qualitative', '2 - 0. Mitigation',\n",
       "       '2 - 1. Adaptation', '3 - 0. Not policy related',\n",
       "       '3 - 1. Analysis of an existing policy / concrete proposal',\n",
       "       ...\n",
       "       '5 - 4.17. Government administration & management',\n",
       "       'representative_sample', 'representative_relevant',\n",
       "       '17 - 0. Supranational and international', '17 - 1. National',\n",
       "       '17 - 2. Sub-national', '19 - 0. Ex-post', '19 - 1. Ex-ante', 'id',\n",
       "       'seen'],\n",
       "      dtype='object', length=110)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.04878383, 0.94671875],\n",
       "       [0.10026041, 0.93964493],\n",
       "       [0.88013387, 0.46886945],\n",
       "       [0.87324774, 0.49748972],\n",
       "       [0.9522103 , 0.06780828],\n",
       "       [0.2017804 , 0.9204664 ],\n",
       "       [0.9539371 , 0.081968  ],\n",
       "       [0.5123893 , 0.8424988 ],\n",
       "       [0.05071485, 0.9505645 ],\n",
       "       [0.9490737 , 0.07786987],\n",
       "       [0.04841587, 0.95061326],\n",
       "       [0.20563701, 0.9146551 ],\n",
       "       [0.06344476, 0.90464824],\n",
       "       [0.43779713, 0.8575195 ],\n",
       "       [0.04716782, 0.9484135 ],\n",
       "       [0.04669519, 0.95064443],\n",
       "       [0.89143884, 0.25497139],\n",
       "       [0.28399476, 0.87579966],\n",
       "       [0.9291732 , 0.09779079],\n",
       "       [0.05733116, 0.9488737 ],\n",
       "       [0.7408345 , 0.70751786],\n",
       "       [0.04471415, 0.94614255],\n",
       "       [0.05353111, 0.95037484],\n",
       "       [0.95169383, 0.08496623],\n",
       "       [0.04830634, 0.9492611 ],\n",
       "       [0.04773958, 0.94849885],\n",
       "       [0.94778293, 0.06458968],\n",
       "       [0.94711417, 0.13433097],\n",
       "       [0.95143455, 0.071148  ],\n",
       "       [0.13258663, 0.9123046 ],\n",
       "       [0.04494832, 0.9473349 ],\n",
       "       [0.04616876, 0.95014167],\n",
       "       [0.94412977, 0.07409178],\n",
       "       [0.9461254 , 0.10849255],\n",
       "       [0.04881271, 0.94735813],\n",
       "       [0.9411544 , 0.07439066],\n",
       "       [0.04803909, 0.9459017 ],\n",
       "       [0.9541923 , 0.08929817],\n",
       "       [0.06453001, 0.9482471 ],\n",
       "       [0.05147925, 0.94992065],\n",
       "       [0.17048311, 0.920965  ],\n",
       "       [0.95156544, 0.06917559],\n",
       "       [0.06567644, 0.9466239 ],\n",
       "       [0.04577176, 0.9432896 ],\n",
       "       [0.1494451 , 0.9168501 ],\n",
       "       [0.04689295, 0.9505524 ],\n",
       "       [0.05174959, 0.9497017 ],\n",
       "       [0.9257357 , 0.13152099],\n",
       "       [0.2757488 , 0.876801  ],\n",
       "       [0.05733116, 0.9488737 ],\n",
       "       [0.05891486, 0.94834536],\n",
       "       [0.90820086, 0.19328761],\n",
       "       [0.12190832, 0.9319935 ],\n",
       "       [0.87388134, 0.41532218],\n",
       "       [0.05185978, 0.9495738 ],\n",
       "       [0.14808229, 0.9334241 ],\n",
       "       [0.47019374, 0.8518661 ],\n",
       "       [0.67328733, 0.6845353 ],\n",
       "       [0.04616876, 0.95014167],\n",
       "       [0.83675253, 0.49824843],\n",
       "       [0.93885136, 0.17031719],\n",
       "       [0.9526669 , 0.07437826],\n",
       "       [0.05100459, 0.95131177],\n",
       "       [0.06395683, 0.9294494 ],\n",
       "       [0.95156544, 0.06917559],\n",
       "       [0.04581112, 0.9478758 ],\n",
       "       [0.95039487, 0.08008453],\n",
       "       [0.12508005, 0.87626094],\n",
       "       [0.8769166 , 0.24790323],\n",
       "       [0.05934052, 0.94901043],\n",
       "       [0.9500531 , 0.09876432],\n",
       "       [0.94276196, 0.15510666],\n",
       "       [0.9307144 , 0.21426477],\n",
       "       [0.04616876, 0.95014167],\n",
       "       [0.6431436 , 0.7286818 ],\n",
       "       [0.0478471 , 0.9510464 ],\n",
       "       [0.857549  , 0.5283776 ],\n",
       "       [0.07926022, 0.9425491 ],\n",
       "       [0.05064436, 0.95096827],\n",
       "       [0.8518845 , 0.5409895 ],\n",
       "       [0.04679872, 0.95044935],\n",
       "       [0.8656853 , 0.46616006],\n",
       "       [0.4448718 , 0.71911114],\n",
       "       [0.3132424 , 0.7140294 ],\n",
       "       [0.7835174 , 0.66847587],\n",
       "       [0.04627613, 0.9478515 ],\n",
       "       [0.11356913, 0.93772274],\n",
       "       [0.04872776, 0.9460723 ],\n",
       "       [0.9159505 , 0.06909133],\n",
       "       [0.79252505, 0.61139214],\n",
       "       [0.04626793, 0.94840866],\n",
       "       [0.04616876, 0.95014167],\n",
       "       [0.06539243, 0.9438446 ],\n",
       "       [0.05309668, 0.95136136],\n",
       "       [0.80509466, 0.55863976],\n",
       "       [0.8712804 , 0.50124466],\n",
       "       [0.04908358, 0.9512728 ],\n",
       "       [0.8897909 , 0.374524  ],\n",
       "       [0.9422856 , 0.08822922],\n",
       "       [0.12654848, 0.9288151 ],\n",
       "       [0.93723285, 0.10065407],\n",
       "       [0.4728234 , 0.8221474 ],\n",
       "       [0.9431637 , 0.13300918],\n",
       "       [0.9470319 , 0.11833056],\n",
       "       [0.05392233, 0.94421935],\n",
       "       [0.3014219 , 0.8107043 ],\n",
       "       [0.04632266, 0.95039177],\n",
       "       [0.21149673, 0.89988565],\n",
       "       [0.04773958, 0.94849885],\n",
       "       [0.93078506, 0.13743143],\n",
       "       [0.9299645 , 0.25346053],\n",
       "       [0.9158204 , 0.29456162],\n",
       "       [0.06408877, 0.9169254 ],\n",
       "       [0.87588984, 0.46897006],\n",
       "       [0.04595716, 0.9495022 ],\n",
       "       [0.8773709 , 0.35330096],\n",
       "       [0.95333976, 0.0819485 ],\n",
       "       [0.91062874, 0.3293678 ],\n",
       "       [0.92730534, 0.20469561],\n",
       "       [0.22606048, 0.8851127 ],\n",
       "       [0.610288  , 0.8001745 ],\n",
       "       [0.92550606, 0.26778138],\n",
       "       [0.22576614, 0.9051252 ],\n",
       "       [0.22500709, 0.9201629 ],\n",
       "       [0.10262273, 0.9367145 ],\n",
       "       [0.84691525, 0.35562664],\n",
       "       [0.08305683, 0.9410159 ],\n",
       "       [0.93774486, 0.15861526],\n",
       "       [0.91817194, 0.21775186],\n",
       "       [0.7960455 , 0.6399378 ],\n",
       "       [0.94759214, 0.09969836],\n",
       "       [0.07554452, 0.9468517 ],\n",
       "       [0.9181834 , 0.11578686],\n",
       "       [0.09299929, 0.9067594 ],\n",
       "       [0.9528626 , 0.0832779 ],\n",
       "       [0.0546707 , 0.9512276 ],\n",
       "       [0.94565725, 0.10682648],\n",
       "       [0.94887865, 0.06986152],\n",
       "       [0.04652208, 0.9502994 ],\n",
       "       [0.12309843, 0.9394285 ],\n",
       "       [0.93988866, 0.11605079],\n",
       "       [0.87610245, 0.4655023 ],\n",
       "       [0.851846  , 0.2669171 ],\n",
       "       [0.15620351, 0.92571455],\n",
       "       [0.8969126 , 0.36908382],\n",
       "       [0.95112675, 0.0986039 ],\n",
       "       [0.0459292 , 0.94846195],\n",
       "       [0.7977961 , 0.62470526],\n",
       "       [0.91817194, 0.21775186],\n",
       "       [0.1278113 , 0.93462116],\n",
       "       [0.9522676 , 0.07771438],\n",
       "       [0.11092805, 0.9200348 ],\n",
       "       [0.07216579, 0.94590604],\n",
       "       [0.0491104 , 0.95083255],\n",
       "       [0.0513565 , 0.95155066],\n",
       "       [0.20858206, 0.880449  ],\n",
       "       [0.9352403 , 0.2052011 ],\n",
       "       [0.06082214, 0.9505476 ],\n",
       "       [0.04616876, 0.95014167],\n",
       "       [0.79438096, 0.5555037 ],\n",
       "       [0.05784736, 0.94395125],\n",
       "       [0.04667196, 0.9457885 ],\n",
       "       [0.04693213, 0.9493781 ],\n",
       "       [0.05629476, 0.9411064 ],\n",
       "       [0.09148882, 0.94225806],\n",
       "       [0.9331746 , 0.23771283],\n",
       "       [0.8920926 , 0.35087785],\n",
       "       [0.06696206, 0.9482326 ],\n",
       "       [0.09270675, 0.944103  ],\n",
       "       [0.3418247 , 0.89864725],\n",
       "       [0.67328733, 0.6845353 ],\n",
       "       [0.05352124, 0.9494989 ],\n",
       "       [0.8847867 , 0.20940085],\n",
       "       [0.30472782, 0.87903976],\n",
       "       [0.09729493, 0.93183607],\n",
       "       [0.9539371 , 0.081968  ],\n",
       "       [0.06254636, 0.9473965 ],\n",
       "       [0.05417353, 0.9483069 ],\n",
       "       [0.63242465, 0.7821397 ],\n",
       "       [0.09757172, 0.9413538 ],\n",
       "       [0.06108051, 0.94879276],\n",
       "       [0.35743007, 0.7094247 ],\n",
       "       [0.04971927, 0.94962883],\n",
       "       [0.08084282, 0.9429916 ],\n",
       "       [0.04939672, 0.94979304],\n",
       "       [0.0537869 , 0.9484374 ],\n",
       "       [0.05034724, 0.94904643],\n",
       "       [0.04789395, 0.94799715],\n",
       "       [0.75209594, 0.6798215 ],\n",
       "       [0.10183898, 0.92810345],\n",
       "       [0.14429702, 0.9255476 ],\n",
       "       [0.9064224 , 0.3284595 ],\n",
       "       [0.04716782, 0.9484135 ],\n",
       "       [0.05033078, 0.951296  ],\n",
       "       [0.05943098, 0.9491962 ],\n",
       "       [0.4225452 , 0.8066889 ],\n",
       "       [0.9126768 , 0.26396698],\n",
       "       [0.88322705, 0.37277374],\n",
       "       [0.05069062, 0.9507707 ],\n",
       "       [0.71576256, 0.3580234 ],\n",
       "       [0.17320193, 0.9250976 ],\n",
       "       [0.13104424, 0.8746001 ],\n",
       "       [0.9227289 , 0.20136766],\n",
       "       [0.9492471 , 0.06471311],\n",
       "       [0.91251004, 0.32440776],\n",
       "       [0.93389714, 0.07148957],\n",
       "       [0.9205966 , 0.30307937],\n",
       "       [0.04505971, 0.94855803],\n",
       "       [0.08745129, 0.9418326 ],\n",
       "       [0.8643275 , 0.31439206],\n",
       "       [0.17561448, 0.7795953 ],\n",
       "       [0.95234424, 0.09556803],\n",
       "       [0.06257141, 0.9477793 ],\n",
       "       [0.06441518, 0.9430006 ],\n",
       "       [0.94201416, 0.15263325],\n",
       "       [0.94412977, 0.07409178],\n",
       "       [0.6574873 , 0.7233074 ],\n",
       "       [0.04878383, 0.94671875],\n",
       "       [0.54304624, 0.7740624 ],\n",
       "       [0.05361764, 0.9489498 ],\n",
       "       [0.04913117, 0.9509036 ],\n",
       "       [0.05119286, 0.9400041 ],\n",
       "       [0.04796913, 0.947888  ],\n",
       "       [0.849384  , 0.5005709 ],\n",
       "       [0.81894314, 0.5712761 ],\n",
       "       [0.04872776, 0.9460723 ],\n",
       "       [0.86315674, 0.49318847],\n",
       "       [0.04934008, 0.95009786],\n",
       "       [0.20875019, 0.91511786],\n",
       "       [0.3327222 , 0.88807595],\n",
       "       [0.93263876, 0.10956646],\n",
       "       [0.10653234, 0.9335655 ],\n",
       "       [0.30477658, 0.90086985],\n",
       "       [0.8568676 , 0.53952074],\n",
       "       [0.04535007, 0.94589376],\n",
       "       [0.93489105, 0.22322984],\n",
       "       [0.60253257, 0.7822457 ],\n",
       "       [0.05597355, 0.9506067 ],\n",
       "       [0.6444767 , 0.7884801 ],\n",
       "       [0.05267024, 0.9483503 ],\n",
       "       [0.9391238 , 0.11276206],\n",
       "       [0.5253497 , 0.8323424 ],\n",
       "       [0.05591587, 0.9507177 ],\n",
       "       [0.04679872, 0.95044935],\n",
       "       [0.2970389 , 0.89066577],\n",
       "       [0.7777352 , 0.65578324],\n",
       "       [0.05943098, 0.9491962 ],\n",
       "       [0.939618  , 0.13422479],\n",
       "       [0.05293737, 0.9490604 ],\n",
       "       [0.0923979 , 0.91074264],\n",
       "       [0.05733116, 0.9488737 ],\n",
       "       [0.04957628, 0.9474192 ],\n",
       "       [0.05369996, 0.9501052 ],\n",
       "       [0.94935995, 0.11160772],\n",
       "       [0.3274919 , 0.7823301 ],\n",
       "       [0.04908358, 0.9512728 ],\n",
       "       [0.9004403 , 0.38270196],\n",
       "       [0.14284481, 0.9279797 ],\n",
       "       [0.05556987, 0.9495766 ],\n",
       "       [0.5948976 , 0.80349797],\n",
       "       [0.9369119 , 0.07515216],\n",
       "       [0.07647048, 0.94470733],\n",
       "       [0.7761609 , 0.6809895 ],\n",
       "       [0.93726146, 0.18129548],\n",
       "       [0.9528626 , 0.0832779 ],\n",
       "       [0.6674364 , 0.77339727],\n",
       "       [0.04558761, 0.9491262 ],\n",
       "       [0.8447855 , 0.40915442],\n",
       "       [0.92334896, 0.26422366],\n",
       "       [0.04616876, 0.95014167],\n",
       "       [0.8099379 , 0.6183077 ],\n",
       "       [0.1252469 , 0.93285096],\n",
       "       [0.8464123 , 0.29783377],\n",
       "       [0.08797217, 0.9425525 ],\n",
       "       [0.7472671 , 0.436558  ],\n",
       "       [0.7762081 , 0.63136333]], dtype=float32)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred = np.load(\"cv_results/bert-tiny__19 -__1__outer_predictions.npy\")\n",
    "y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1., 2., 0.])"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[targets].sum(axis=1).unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2513, 105)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>title</th>\n",
       "      <th>content</th>\n",
       "      <th>labels</th>\n",
       "      <th>random</th>\n",
       "      <th>INCLUDE</th>\n",
       "      <th>sample_weight</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>3778807</td>\n",
       "      <td>When salient science is not enough to advance ...</td>\n",
       "      <td>Increased social and environmental vulnerabili...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>819936</td>\n",
       "      <td>Planning for variable renewable energy and ele...</td>\n",
       "      <td>High urbanization rates, decentralized solar p...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.666667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>212843</td>\n",
       "      <td>Modeling climate change and agriculture: an in...</td>\n",
       "      <td>This issue of Agricultural Economics is a spec...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>791223</td>\n",
       "      <td>Environmental benefits from ridesharing: A cas...</td>\n",
       "      <td>Emerging ridesharing travel could be an effect...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>435782</td>\n",
       "      <td>A viable technology to generate third-generati...</td>\n",
       "      <td>First generation biofuels are widely available...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        id                                              title  \\\n",
       "0  3778807  When salient science is not enough to advance ...   \n",
       "1   819936  Planning for variable renewable energy and ele...   \n",
       "2   212843  Modeling climate change and agriculture: an in...   \n",
       "3   791223  Environmental benefits from ridesharing: A cas...   \n",
       "4   435782  A viable technology to generate third-generati...   \n",
       "\n",
       "                                             content  labels  random  INCLUDE  \\\n",
       "0  Increased social and environmental vulnerabili...       0       0      0.0   \n",
       "1  High urbanization rates, decentralized solar p...       0       0      0.0   \n",
       "2  This issue of Agricultural Economics is a spec...       0       0      0.0   \n",
       "3  Emerging ridesharing travel could be an effect...       0       0      0.0   \n",
       "4  First generation biofuels are widely available...       0       0      0.0   \n",
       "\n",
       "   sample_weight  \n",
       "0       1.000000  \n",
       "1       0.666667  \n",
       "2       1.000000  \n",
       "3       1.000000  \n",
       "4       1.000000  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "args.y_prefix = \"4 - \"\n",
    "args.y_prefix = \"INCLUDE\"\n",
    "#args.y_prefix = \"8 - \"\n",
    "args.n_splits = 3\n",
    "args.make_predictions = \"False\"\n",
    "args.roundup=False\n",
    "cols = [x for x in seen_df.columns if re.match(f\"^{args.y_prefix}\",x)]\n",
    "\n",
    "\n",
    "# If the target is inclusion, use only those documents for which we have a non-na value\n",
    "# Otherwise, only use those documents which are included\n",
    "# also define what subset is to be treated as a random representative sample\n",
    "# For labels beyond inclusion, we treat all those that are representative of the included\n",
    "# studies as representative\n",
    "if \"INCLUDE\" in args.y_prefix:\n",
    "    y_var = cols[0]\n",
    "    seen_df = seen_df.loc[pd.notna(seen_df[y_var]),:].reset_index(drop=True)\n",
    "    seen_df['random'] = seen_df['representative_sample']\n",
    "else:\n",
    "    seen_df = seen_df[seen_df['INCLUDE']==1]\n",
    "    seen_df['random'] = seen_df['representative_relevant']\n",
    "\n",
    "# Turn the columns into target variables and get class-weights to counteract class imbalances\n",
    "if len(cols)==1:\n",
    "    y_var = cols[0]\n",
    "    seen_df = seen_df.loc[pd.notna(seen_df[y_var]),:].reset_index(drop=True)\n",
    "    print(seen_df.shape)\n",
    "    seen_df['labels'] = list(seen_df[y_var].values.astype(int))\n",
    "    cw = seen_df[(seen_df['random']==1) & (seen_df[y_var]==0)].shape[0] / seen_df[(seen_df['random']==1) & (seen_df[y_var]==1)].shape[0]\n",
    "    class_weight={1:cw}\n",
    "    scorer = \"F1\"\n",
    "    weights_df[\"sample_weight\"] = list(weights_df[y_var+\"_sample_weight\"].fillna(1).values)\n",
    "else:\n",
    "    num_labels = len(cols) \n",
    "    weights_df['sample_weight'] = list(weights_df[[x+\"_sample_weight\" for x in cols]].fillna(1).values)\n",
    "    seen_df = seen_df.replace(2,1)\n",
    "    seen_df['labels'] = list(seen_df[cols].values.astype(int))\n",
    "    seen_df = seen_df.dropna(subset=cols)\n",
    "    seen_df = seen_df.reset_index(drop=True)\n",
    "    scorer = \"F1 macro\"\n",
    "    class_weight = {}\n",
    "    for i, t in enumerate(cols):\n",
    "        cw = seen_df[(seen_df['random']==1) & (seen_df[t]==0)].shape[0] / seen_df[(seen_df['random']==1) & (seen_df[t]==1)].shape[0]\n",
    "        class_weight[i] = cw\n",
    "\n",
    "# Remove unneccessary columns\n",
    "seen_df = seen_df[[\"id\",\"title\",\"content\",\"labels\",\"random\"]+cols].merge(\n",
    "    weights_df[[\"doc__id\",\"sample_weight\"]].rename(columns={\"doc__id\":\"id\"})\n",
    ")\n",
    "seen_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge with the unseen data if necessary                                                                \n",
    "seen_df['seen']  = 1\n",
    "if args.make_predictions==\"True\":\n",
    "    unseen_df = pd.read_csv('../data/0_unlabelled_documents.csv')\n",
    "    unseen_df['seen'] = 0\n",
    "    df = (pd.concat([seen_df,unseen_df])\n",
    "          .sort_values('id')\n",
    "          .sample(frac=1, random_state=1)\n",
    "          .reset_index(drop=True)\n",
    "    )\n",
    "    df.content = df.content.astype(str)\n",
    "else:\n",
    "    df = seen_df\n",
    "    \n",
    "nonrandom_index = df[(df['random']!=1) & (df['seen']==1)].index\n",
    "random_index = df[df['random']==1].index\n",
    "seen_index = df[df['seen']==1].index\n",
    "unseen_index = df[df['seen']==0].index\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2513, 8)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'climatebert/distilroberta-base-climate-f'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "args.model_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2271 242\n",
      "{'class_weight': {1: 14.954545454545455}, 'sample_weighted': True, 'batch_size': 8, 'weight_decay': 0, 'learning_rate': 2e-05, 'num_epochs': 4}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at climatebert/distilroberta-base-climate-f were not used when initializing RobertaForSequenceClassification: ['lm_head.bias', 'lm_head.layer_norm.weight', 'lm_head.layer_norm.bias', 'lm_head.dense.bias', 'lm_head.dense.weight']\n",
      "- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at climatebert/distilroberta-base-climate-f and are newly initialized: ['classifier.dense.weight', 'classifier.dense.bias', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "could not put model on GPU\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/max/.local/lib/python3.6/site-packages/torch/cuda/__init__.py:52: UserWarning: CUDA initialization: Found no NVIDIA driver on your system. Please check that you have an NVIDIA GPU and installed a driver from http://www.nvidia.com/Download/index.aspx (Triggered internally at  /pytorch/c10/cuda/CUDAFunctions.cpp:100.)\n",
      "  return torch._C._cuda_getDeviceCount() > 0\n",
      "***** Running training *****\n",
      "  Num examples = 2058\n",
      "  Num Epochs = 4\n",
      "  Instantaneous batch size per device = 8\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 8\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 1032\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training bert with these params\n",
      "{'class_weight': {1: 14.954545454545455}, 'sample_weighted': True, 'batch_size': 8, 'weight_decay': 0, 'learning_rate': 2e-05, 'num_epochs': 4}\n",
      "torch.Size([8, 1]) torch.Size([8])\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='221' max='1032' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [ 221/1032 1:37:53 < 6:02:31, 0.04 it/s, Epoch 0.85/4]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([8, 1]) torch.Size([8])\n",
      "torch.Size([8, 1]) torch.Size([8])\n",
      "torch.Size([8, 1]) torch.Size([8])\n",
      "torch.Size([8, 1]) torch.Size([8])\n",
      "torch.Size([8, 1]) torch.Size([8])\n",
      "torch.Size([8, 1]) torch.Size([8])\n",
      "torch.Size([8, 1]) torch.Size([8])\n",
      "torch.Size([8, 1]) torch.Size([8])\n",
      "torch.Size([8, 1]) torch.Size([8])\n",
      "torch.Size([8, 1]) torch.Size([8])\n",
      "torch.Size([8, 1]) torch.Size([8])\n",
      "torch.Size([8, 1]) torch.Size([8])\n",
      "torch.Size([8, 1]) torch.Size([8])\n",
      "torch.Size([8, 1]) torch.Size([8])\n",
      "torch.Size([8, 1]) torch.Size([8])\n",
      "torch.Size([8, 1]) torch.Size([8])\n",
      "torch.Size([8, 1]) torch.Size([8])\n",
      "torch.Size([8, 1]) torch.Size([8])\n",
      "torch.Size([8, 1]) torch.Size([8])\n",
      "torch.Size([8, 1]) torch.Size([8])\n",
      "torch.Size([8, 1]) torch.Size([8])\n",
      "torch.Size([8, 1]) torch.Size([8])\n",
      "torch.Size([8, 1]) torch.Size([8])\n",
      "torch.Size([8, 1]) torch.Size([8])\n",
      "torch.Size([8, 1]) torch.Size([8])\n",
      "torch.Size([8, 1]) torch.Size([8])\n",
      "torch.Size([8, 1]) torch.Size([8])\n",
      "torch.Size([8, 1]) torch.Size([8])\n",
      "torch.Size([8, 1]) torch.Size([8])\n",
      "torch.Size([8, 1]) torch.Size([8])\n",
      "torch.Size([8, 1]) torch.Size([8])\n",
      "torch.Size([8, 1]) torch.Size([8])\n",
      "torch.Size([8, 1]) torch.Size([8])\n",
      "torch.Size([8, 1]) torch.Size([8])\n",
      "torch.Size([8, 1]) torch.Size([8])\n",
      "torch.Size([8, 1]) torch.Size([8])\n",
      "torch.Size([8, 1]) torch.Size([8])\n",
      "torch.Size([8, 1]) torch.Size([8])\n",
      "torch.Size([8, 1]) torch.Size([8])\n",
      "torch.Size([8, 1]) torch.Size([8])\n",
      "torch.Size([8, 1]) torch.Size([8])\n",
      "torch.Size([8, 1]) torch.Size([8])\n",
      "torch.Size([8, 1]) torch.Size([8])\n",
      "torch.Size([8, 1]) torch.Size([8])\n",
      "torch.Size([8, 1]) torch.Size([8])\n",
      "torch.Size([8, 1]) torch.Size([8])\n",
      "torch.Size([8, 1]) torch.Size([8])\n",
      "torch.Size([8, 1]) torch.Size([8])\n",
      "torch.Size([8, 1]) torch.Size([8])\n",
      "torch.Size([8, 1]) torch.Size([8])\n",
      "torch.Size([8, 1]) torch.Size([8])\n",
      "torch.Size([8, 1]) torch.Size([8])\n",
      "torch.Size([8, 1]) torch.Size([8])\n",
      "torch.Size([8, 1]) torch.Size([8])\n",
      "torch.Size([8, 1]) torch.Size([8])\n",
      "torch.Size([8, 1]) torch.Size([8])\n",
      "torch.Size([8, 1]) torch.Size([8])\n",
      "torch.Size([8, 1]) torch.Size([8])\n",
      "torch.Size([8, 1]) torch.Size([8])\n",
      "torch.Size([8, 1]) torch.Size([8])\n",
      "torch.Size([8, 1]) torch.Size([8])\n",
      "torch.Size([8, 1]) torch.Size([8])\n",
      "torch.Size([8, 1]) torch.Size([8])\n",
      "torch.Size([8, 1]) torch.Size([8])\n",
      "torch.Size([8, 1]) torch.Size([8])\n",
      "torch.Size([8, 1]) torch.Size([8])\n",
      "torch.Size([8, 1]) torch.Size([8])\n",
      "torch.Size([8, 1]) torch.Size([8])\n",
      "torch.Size([8, 1]) torch.Size([8])\n",
      "torch.Size([8, 1]) torch.Size([8])\n",
      "torch.Size([8, 1]) torch.Size([8])\n",
      "torch.Size([8, 1]) torch.Size([8])\n",
      "torch.Size([8, 1]) torch.Size([8])\n",
      "torch.Size([8, 1]) torch.Size([8])\n",
      "torch.Size([8, 1]) torch.Size([8])\n",
      "torch.Size([8, 1]) torch.Size([8])\n",
      "torch.Size([8, 1]) torch.Size([8])\n",
      "torch.Size([8, 1]) torch.Size([8])\n",
      "torch.Size([8, 1]) torch.Size([8])\n",
      "torch.Size([8, 1]) torch.Size([8])\n",
      "torch.Size([8, 1]) torch.Size([8])\n",
      "torch.Size([8, 1]) torch.Size([8])\n",
      "torch.Size([8, 1]) torch.Size([8])\n",
      "torch.Size([8, 1]) torch.Size([8])\n",
      "torch.Size([8, 1]) torch.Size([8])\n",
      "torch.Size([8, 1]) torch.Size([8])\n",
      "torch.Size([8, 1]) torch.Size([8])\n",
      "torch.Size([8, 1]) torch.Size([8])\n",
      "torch.Size([8, 1]) torch.Size([8])\n",
      "torch.Size([8, 1]) torch.Size([8])\n",
      "torch.Size([8, 1]) torch.Size([8])\n",
      "torch.Size([8, 1]) torch.Size([8])\n",
      "torch.Size([8, 1]) torch.Size([8])\n",
      "torch.Size([8, 1]) torch.Size([8])\n",
      "torch.Size([8, 1]) torch.Size([8])\n",
      "torch.Size([8, 1]) torch.Size([8])\n",
      "torch.Size([8, 1]) torch.Size([8])\n",
      "torch.Size([8, 1]) torch.Size([8])\n",
      "torch.Size([8, 1]) torch.Size([8])\n",
      "torch.Size([8, 1]) torch.Size([8])\n",
      "torch.Size([8, 1]) torch.Size([8])\n",
      "torch.Size([8, 1]) torch.Size([8])\n",
      "torch.Size([8, 1]) torch.Size([8])\n",
      "torch.Size([8, 1]) torch.Size([8])\n",
      "torch.Size([8, 1]) torch.Size([8])\n",
      "torch.Size([8, 1]) torch.Size([8])\n",
      "torch.Size([8, 1]) torch.Size([8])\n",
      "torch.Size([8, 1]) torch.Size([8])\n",
      "torch.Size([8, 1]) torch.Size([8])\n",
      "torch.Size([8, 1]) torch.Size([8])\n",
      "torch.Size([8, 1]) torch.Size([8])\n",
      "torch.Size([8, 1]) torch.Size([8])\n",
      "torch.Size([8, 1]) torch.Size([8])\n",
      "torch.Size([8, 1]) torch.Size([8])\n",
      "torch.Size([8, 1]) torch.Size([8])\n",
      "torch.Size([8, 1]) torch.Size([8])\n",
      "torch.Size([8, 1]) torch.Size([8])\n",
      "torch.Size([8, 1]) torch.Size([8])\n",
      "torch.Size([8, 1]) torch.Size([8])\n",
      "torch.Size([8, 1]) torch.Size([8])\n",
      "torch.Size([8, 1]) torch.Size([8])\n",
      "torch.Size([8, 1]) torch.Size([8])\n",
      "torch.Size([8, 1]) torch.Size([8])\n",
      "torch.Size([8, 1]) torch.Size([8])\n",
      "torch.Size([8, 1]) torch.Size([8])\n",
      "torch.Size([8, 1]) torch.Size([8])\n",
      "torch.Size([8, 1]) torch.Size([8])\n",
      "torch.Size([8, 1]) torch.Size([8])\n",
      "torch.Size([8, 1]) torch.Size([8])\n",
      "torch.Size([8, 1]) torch.Size([8])\n",
      "torch.Size([8, 1]) torch.Size([8])\n",
      "torch.Size([8, 1]) torch.Size([8])\n",
      "torch.Size([8, 1]) torch.Size([8])\n",
      "torch.Size([8, 1]) torch.Size([8])\n",
      "torch.Size([8, 1]) torch.Size([8])\n",
      "torch.Size([8, 1]) torch.Size([8])\n",
      "torch.Size([8, 1]) torch.Size([8])\n",
      "torch.Size([8, 1]) torch.Size([8])\n",
      "torch.Size([8, 1]) torch.Size([8])\n",
      "torch.Size([8, 1]) torch.Size([8])\n",
      "torch.Size([8, 1]) torch.Size([8])\n",
      "torch.Size([8, 1]) torch.Size([8])\n",
      "torch.Size([8, 1]) torch.Size([8])\n",
      "torch.Size([8, 1]) torch.Size([8])\n",
      "torch.Size([8, 1]) torch.Size([8])\n",
      "torch.Size([8, 1]) torch.Size([8])\n",
      "torch.Size([8, 1]) torch.Size([8])\n",
      "torch.Size([8, 1]) torch.Size([8])\n",
      "torch.Size([8, 1]) torch.Size([8])\n",
      "torch.Size([8, 1]) torch.Size([8])\n",
      "torch.Size([8, 1]) torch.Size([8])\n",
      "torch.Size([8, 1]) torch.Size([8])\n",
      "torch.Size([8, 1]) torch.Size([8])\n",
      "torch.Size([8, 1]) torch.Size([8])\n",
      "torch.Size([8, 1]) torch.Size([8])\n",
      "torch.Size([8, 1]) torch.Size([8])\n",
      "torch.Size([8, 1]) torch.Size([8])\n",
      "torch.Size([8, 1]) torch.Size([8])\n",
      "torch.Size([8, 1]) torch.Size([8])\n",
      "torch.Size([8, 1]) torch.Size([8])\n",
      "torch.Size([8, 1]) torch.Size([8])\n",
      "torch.Size([8, 1]) torch.Size([8])\n",
      "torch.Size([8, 1]) torch.Size([8])\n",
      "torch.Size([8, 1]) torch.Size([8])\n",
      "torch.Size([8, 1]) torch.Size([8])\n",
      "torch.Size([8, 1]) torch.Size([8])\n",
      "torch.Size([8, 1]) torch.Size([8])\n",
      "torch.Size([8, 1]) torch.Size([8])\n",
      "torch.Size([8, 1]) torch.Size([8])\n",
      "torch.Size([8, 1]) torch.Size([8])\n",
      "torch.Size([8, 1]) torch.Size([8])\n",
      "torch.Size([8, 1]) torch.Size([8])\n",
      "torch.Size([8, 1]) torch.Size([8])\n",
      "torch.Size([8, 1]) torch.Size([8])\n",
      "torch.Size([8, 1]) torch.Size([8])\n",
      "torch.Size([8, 1]) torch.Size([8])\n",
      "torch.Size([8, 1]) torch.Size([8])\n",
      "torch.Size([8, 1]) torch.Size([8])\n",
      "torch.Size([8, 1]) torch.Size([8])\n",
      "torch.Size([8, 1]) torch.Size([8])\n",
      "torch.Size([8, 1]) torch.Size([8])\n",
      "torch.Size([8, 1]) torch.Size([8])\n",
      "torch.Size([8, 1]) torch.Size([8])\n",
      "torch.Size([8, 1]) torch.Size([8])\n",
      "torch.Size([8, 1]) torch.Size([8])\n",
      "torch.Size([8, 1]) torch.Size([8])\n",
      "torch.Size([8, 1]) torch.Size([8])\n",
      "torch.Size([8, 1]) torch.Size([8])\n",
      "torch.Size([8, 1]) torch.Size([8])\n",
      "torch.Size([8, 1]) torch.Size([8])\n",
      "torch.Size([8, 1]) torch.Size([8])\n",
      "torch.Size([8, 1]) torch.Size([8])\n",
      "torch.Size([8, 1]) torch.Size([8])\n",
      "torch.Size([8, 1]) torch.Size([8])\n",
      "torch.Size([8, 1]) torch.Size([8])\n",
      "torch.Size([8, 1]) torch.Size([8])\n",
      "torch.Size([8, 1]) torch.Size([8])\n",
      "torch.Size([8, 1]) torch.Size([8])\n",
      "torch.Size([8, 1]) torch.Size([8])\n",
      "torch.Size([8, 1]) torch.Size([8])\n",
      "torch.Size([8, 1]) torch.Size([8])\n",
      "torch.Size([8, 1]) torch.Size([8])\n",
      "torch.Size([8, 1]) torch.Size([8])\n",
      "torch.Size([8, 1]) torch.Size([8])\n",
      "torch.Size([8, 1]) torch.Size([8])\n",
      "torch.Size([8, 1]) torch.Size([8])\n",
      "torch.Size([8, 1]) torch.Size([8])\n",
      "torch.Size([8, 1]) torch.Size([8])\n",
      "torch.Size([8, 1]) torch.Size([8])\n",
      "torch.Size([8, 1]) torch.Size([8])\n",
      "torch.Size([8, 1]) torch.Size([8])\n",
      "torch.Size([8, 1]) torch.Size([8])\n",
      "torch.Size([8, 1]) torch.Size([8])\n",
      "torch.Size([8, 1]) torch.Size([8])\n",
      "torch.Size([8, 1]) torch.Size([8])\n",
      "torch.Size([8, 1]) torch.Size([8])\n",
      "torch.Size([8, 1]) torch.Size([8])\n",
      "torch.Size([8, 1]) torch.Size([8])\n",
      "torch.Size([8, 1]) torch.Size([8])\n"
     ]
    }
   ],
   "source": [
    "# import the helper functions defined in cv_setup.py\n",
    "import cv_setup as cvs\n",
    "\n",
    "# Get the BERT parameters, and include class_weight as a parameter to be tested\n",
    "bert_params = cvs.bert_params\n",
    "bert_params['class_weight'].append(class_weight)\n",
    "bert_params['class_weight'] = [class_weight]\n",
    "param_space = list(cvs.product_dict(**bert_params))\n",
    "params = list(bert_params.keys())\n",
    "\n",
    "test = False\n",
    "if test:\n",
    "    param_space = param_space[:2]\n",
    "\n",
    "# Get the spilts for our outer fold, discard=False means to pass the nonrandom documents that\n",
    "# would ordinarily be in the validation set back into the test set\n",
    "outer_cv = cvs.KFoldRandom(args.n_splits, seen_index, nonrandom_index, discard=False)\n",
    "\n",
    "# Iterate through the folds\n",
    "for k, (train, test) in enumerate(outer_cv):    \n",
    "    print(len(train), len(test))\n",
    "    cv_results = []\n",
    "    inner_cv = cvs.KFoldRandom(args.n_splits, train, nonrandom_index, discard=False)\n",
    "    inner_scores = []\n",
    "    for l, (l_train, l_test) in enumerate(inner_cv):\n",
    "        for pr in param_space:\n",
    "            pr['batch_size'] = 8\n",
    "            print(pr)\n",
    "            cv_results.append(cvs.train_eval_bert(\n",
    "                args.model_name, pr, df=df, targets=cols, train=l_train, test=l_test, \n",
    "                roundup=args.roundup\n",
    "            ))\n",
    "            break\n",
    "            w = df.loc[l_test,'sample_weight']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "len(torch.zeros(8).unsqueeze(1).shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import itertools\n",
    "\n",
    "def product_dict(**kwargs):\n",
    "    keys = kwargs.keys()\n",
    "    vals = kwargs.values()\n",
    "    for instance in itertools.product(*vals):\n",
    "        yield dict(zip(keys, instance))\n",
    "\n",
    "bert_params = {\n",
    "  \"class_weight\": [None],\n",
    "  \"batch_size\": [16, 32],\n",
    "  \"weight_decay\": (0, 0.3),\n",
    "  \"learning_rate\": (1e-5, 5e-5),\n",
    "  \"num_epochs\": [2, 3, 4]\n",
    "}\n",
    "bert_params['class_weight'].append(class_weight)\n",
    "params = list(bert_params.keys())\n",
    "print(params)\n",
    "param_space = list(product_dict(**bert_params))\n",
    "scorer = \"F1\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, BertTokenizer, AutoModel, trainer, BertForSequenceClassification\n",
    "params = bert_params\n",
    "\n",
    "model_name = \"allenai/scibert_scivocab_uncased\"\n",
    "\n",
    "model = BertForSequenceClassification.from_pretrained(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "params = param_space[0]\n",
    "params['class_weight'] = {0: 1, 1: 6.831460674157303}\n",
    "params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = BertTokenizer.from_pretrained(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.head(500)\n",
    "from cv_setup import create_train_val\n",
    "train = df[:100]\n",
    "test = df[100:200]\n",
    "#df = df.dropna()\n",
    "#df['labels'] = df.INCLUDE\n",
    "train_dataset, val_dataset, MAX_LEN = create_train_val(\n",
    "    tokenizer, df['content'].astype(str), \n",
    "    df['labels'], \n",
    "    df.index[0:100], \n",
    "    df.index[100:200], \n",
    "    tensorflow=False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "train_index = df.index[0:10]\n",
    "test_index = df.index[100:200]\n",
    "\n",
    "x = df['content']\n",
    "y = df['labels']\n",
    "\n",
    "train_encodings= tokenizer(list(x[train_index]),truncation=True,padding=True,max_length=512)\n",
    "val_encodings = tokenizer(list(x[test_index]),truncation=True,padding=True,max_length=512)\n",
    "\n",
    "class TDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, encodings, labels):\n",
    "        self.encodings = encodings\n",
    "        self.labels = labels\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
    "        item['labels'] = torch.tensor(self.labels[idx],dtype=torch.float32)\n",
    "        return item\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "    \n",
    "train_dataset = TDataset(train_encodings, list(y[train_index]))\n",
    "val_dataset = TDataset(val_encodings, list(y[test_index]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch \n",
    "torch.zeros([16]).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.__version__\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import Trainer, TrainingArguments\n",
    "\n",
    "params = param_space[1]\n",
    "params['batch_size'] = 4\n",
    "params['num_epochs'] = 1\n",
    "params['class_weight'] = class_weight\n",
    "\n",
    "model = BertForSequenceClassification.from_pretrained(model_name, num_labels=num_labels)\n",
    "\n",
    "from transformers import Trainer, TrainingArguments\n",
    "\n",
    "class CWTrainer(Trainer):\n",
    "    def compute_loss(self, model, inputs, return_outputs=False):\n",
    "        labels = inputs.pop(\"labels\")\n",
    "        try:\n",
    "            device = torch.device('cuda:0')\n",
    "        except:\n",
    "            print(\"no device\")\n",
    "        labels = labels.to(device)\n",
    "        \n",
    "        if \"sample_weight\" in inputs.keys():\n",
    "            sample_weight = inputs.pop(\"sample_weight\")\n",
    "        else:\n",
    "            sample_weight = None\n",
    "        outputs = model(**inputs)\n",
    "        logits = outputs.logits\n",
    "\n",
    "        if self.class_weight is not None:\n",
    "            cw = torch.tensor(list(self.class_weight.values()))\n",
    "\n",
    "        else:\n",
    "            cw = None\n",
    "        loss_fct = torch.nn.BCEWithLogitsLoss(pos_weight=cw,reduction='none')\n",
    "        loss = loss_fct(logits,labels.float())\n",
    "        #loss = loss_fct(logits.view(-1, model.num_labels),\n",
    "        #                labels.float().view(-1, model.num_labels))\n",
    "        if sample_weight is not None:\n",
    "            loss = (loss * sample_weight / sample_weight.sum()).sum().mean()\n",
    "        else:\n",
    "            loss = loss.mean()\n",
    "        return (loss, outputs) if return_outputs else loss\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "class BCWTrainer(Trainer):\n",
    "    def compute_loss(self, model, inputs, return_outputs=False):\n",
    "        labels = inputs.pop(\"labels\")\n",
    "        outputs = model(**inputs)\n",
    "        logits = outputs.logits\n",
    "        cw = torch.tensor(self.class_weight[1])\n",
    "        loss_fct = torch.nn.BCEWithLogitsLoss(pos_weight=cw)\n",
    "        loss = loss_fct(logits.view(-1, 1),\n",
    "                        labels.float().view(-1, 1))\n",
    "        return (loss, outputs) if return_outputs else loss\n",
    "\n",
    "\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./results\",\n",
    "    num_train_epochs=params['num_epochs'],\n",
    "    per_device_train_batch_size=params['batch_size'],\n",
    "    learning_rate=params['learning_rate'],\n",
    "    weight_decay=params['weight_decay'],\n",
    "    gradient_checkpointing=True\n",
    ")\n",
    "\n",
    "trainer = CWTrainer(\n",
    "    model=model,\n",
    "    args = training_args, \n",
    "\n",
    "    #optimizers=[optimizer]\n",
    ")\n",
    "trainer.class_weight = params['class_weight']\n",
    "#trainer.class_weight=torch.tensor(list(params[\"class_weight\"].values()))\n",
    "trainer.train_dataset = train_dataset\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "***** Running Prediction *****\n",
      "  Num examples = 100\n",
      "  Batch size = 8\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='13' max='13' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [13/13 00:54]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5\n",
      "5\n",
      "5\n",
      "5\n",
      "5\n",
      "5\n",
      "5\n",
      "5\n",
      "5\n",
      "5\n",
      "5\n",
      "5\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "PredictionOutput(predictions=array([[-8.92750919e-04, -3.03136647e-01, -1.38864815e-01,\n",
       "         6.98384702e-01,  3.95447195e-01],\n",
       "       [ 1.57699585e-01, -5.91032207e-01, -2.06474334e-01,\n",
       "         5.79557359e-01,  4.95105952e-01],\n",
       "       [-1.48357376e-02, -5.85529447e-01,  4.19373810e-02,\n",
       "         7.41156518e-01,  4.59309220e-01],\n",
       "       [-1.32918879e-01, -6.13522530e-01, -3.00120413e-02,\n",
       "         5.59860528e-01,  6.39672160e-01],\n",
       "       [-2.53252238e-02, -2.24090785e-01, -1.12039998e-01,\n",
       "         9.25198257e-01,  1.25383645e-01],\n",
       "       [ 6.51370734e-03, -7.38315657e-02, -2.66591877e-01,\n",
       "         9.57293272e-01,  2.33598292e-01],\n",
       "       [-8.65197778e-02, -5.30148506e-01, -2.62979597e-01,\n",
       "         1.84291363e-01,  3.85540426e-01],\n",
       "       [ 1.28535986e-01, -1.30125344e-01, -2.80426085e-01,\n",
       "         3.67568821e-01,  3.64873350e-01],\n",
       "       [ 3.82994860e-02, -1.16870873e-01, -2.80790508e-01,\n",
       "         1.02078664e+00,  1.74427435e-01],\n",
       "       [-5.51003888e-02, -3.46435606e-01, -2.93871760e-02,\n",
       "         5.97128689e-01,  3.68145138e-01],\n",
       "       [-2.65996456e-02, -6.09190464e-01, -2.98441648e-01,\n",
       "         1.18418440e-01,  4.31630611e-01],\n",
       "       [ 1.68989748e-01, -2.63650715e-01, -2.02785000e-01,\n",
       "         3.99682939e-01,  2.35241085e-01],\n",
       "       [-1.49573624e-01, -1.55825496e-01, -2.13540673e-01,\n",
       "         9.98327017e-01,  3.72696579e-01],\n",
       "       [ 3.97241041e-02, -4.90182221e-01, -4.36564386e-02,\n",
       "         6.02961659e-01,  4.21397746e-01],\n",
       "       [ 3.35476398e-02, -3.23351547e-02, -9.20904428e-02,\n",
       "         5.26952326e-01,  2.21806169e-01],\n",
       "       [ 7.82158673e-02, -3.00411135e-01, -3.49906236e-01,\n",
       "         6.56384706e-01,  4.38566267e-01],\n",
       "       [ 8.37323368e-02, -5.21962047e-01, -6.65513277e-02,\n",
       "         2.98652291e-01,  3.26716214e-01],\n",
       "       [ 1.77166268e-01, -3.53724658e-01, -2.48761654e-01,\n",
       "         4.97530967e-01,  3.42167884e-01],\n",
       "       [ 6.55552000e-03, -4.23032284e-01,  3.63856554e-02,\n",
       "         6.44068837e-01,  3.64523768e-01],\n",
       "       [-9.73768607e-02, -3.93437028e-01, -4.63530123e-02,\n",
       "         7.22429276e-01,  1.42968968e-01],\n",
       "       [ 4.25382406e-02, -5.72657406e-01,  5.37964851e-02,\n",
       "         5.86013675e-01,  3.92513990e-01],\n",
       "       [ 1.69128507e-01, -7.29597807e-02, -3.39588523e-02,\n",
       "         5.85407972e-01,  2.01807827e-01],\n",
       "       [ 2.77249813e-01, -5.81965208e-01, -3.16733122e-03,\n",
       "         4.64447975e-01,  4.41008389e-01],\n",
       "       [-3.96225601e-02, -1.51217520e-01, -1.14525899e-01,\n",
       "         8.78713787e-01,  4.62736011e-01],\n",
       "       [ 8.11126456e-03, -4.59487438e-02, -2.04442590e-01,\n",
       "         7.69175887e-01,  2.17341512e-01],\n",
       "       [ 2.34682560e-02, -2.03644037e-01, -1.30319446e-01,\n",
       "         6.00456774e-01,  2.86684185e-01],\n",
       "       [ 1.37668043e-01, -3.41291338e-01, -7.76116103e-02,\n",
       "         6.10336304e-01,  3.39999437e-01],\n",
       "       [ 1.06338382e-01, -2.89920211e-01, -4.47193384e-02,\n",
       "         7.98386097e-01,  3.21369708e-01],\n",
       "       [ 1.52863503e-01, -5.51974237e-01, -4.49836850e-02,\n",
       "         2.43738800e-01,  3.22782040e-01],\n",
       "       [-1.18790671e-01, -2.02949733e-01, -2.53274918e-01,\n",
       "         6.83721960e-01,  6.35967433e-01],\n",
       "       [ 2.25790471e-01, -1.47066891e-01, -9.71300900e-03,\n",
       "         8.63902867e-01,  3.48440349e-01],\n",
       "       [ 1.30176127e-01, -2.00953901e-01, -1.08821064e-01,\n",
       "         7.31765389e-01,  2.99508065e-01],\n",
       "       [-2.52914131e-02, -4.90415901e-01,  2.71957219e-02,\n",
       "         5.83469272e-01,  4.04473186e-01],\n",
       "       [-4.09077406e-02, -3.22898030e-01, -8.66174698e-03,\n",
       "         8.42162371e-01,  1.22993469e-01],\n",
       "       [ 2.13140920e-02, -1.58853784e-01, -1.89146146e-01,\n",
       "         3.57584774e-01,  2.32151121e-01],\n",
       "       [ 8.97433162e-02, -1.22651160e-01, -3.03443521e-01,\n",
       "         7.25559533e-01,  1.79490283e-01],\n",
       "       [ 5.37973829e-02, -6.43273115e-01,  1.53989464e-01,\n",
       "         5.38337469e-01,  2.49555752e-01],\n",
       "       [-4.84855324e-02, -3.04714203e-01, -9.82748568e-02,\n",
       "         9.45998907e-01,  1.86854511e-01],\n",
       "       [-1.17927045e-02, -4.81918931e-01, -1.58266917e-01,\n",
       "         4.97468233e-01,  1.71036094e-01],\n",
       "       [ 1.87979773e-01, -2.30857641e-01, -2.28235453e-01,\n",
       "         8.57241690e-01,  3.50414962e-01],\n",
       "       [ 1.24447778e-01, -4.38362598e-01, -6.34558201e-02,\n",
       "         5.53708732e-01,  3.12068284e-01],\n",
       "       [ 2.00537175e-01, -1.70080900e-01, -1.62409201e-01,\n",
       "         8.63145232e-01,  2.58141398e-01],\n",
       "       [ 1.08491480e-01, -8.91897917e-01,  2.81951427e-02,\n",
       "         3.95709068e-01,  5.76743007e-01],\n",
       "       [ 2.46976823e-01, -2.21851066e-01, -6.86119646e-02,\n",
       "         7.67821193e-01,  1.14729591e-01],\n",
       "       [ 1.35061920e-01, -4.56980646e-01,  3.86190414e-03,\n",
       "         2.52312809e-01,  3.83647203e-01],\n",
       "       [-3.87779064e-02, -6.93355322e-01,  9.48600918e-02,\n",
       "         4.66520905e-01,  3.75117272e-01],\n",
       "       [-2.93062478e-02, -3.53342116e-01, -8.74334574e-03,\n",
       "         5.77974439e-01,  1.74939111e-01],\n",
       "       [-2.87960470e-02, -2.69800782e-01, -1.75035283e-01,\n",
       "         5.19053102e-01,  3.25485349e-01],\n",
       "       [-2.10925937e-04, -2.08754599e-01, -3.03298533e-01,\n",
       "         6.38261855e-01,  4.18535203e-01],\n",
       "       [-5.61770797e-02, -6.51262283e-01, -4.13245380e-01,\n",
       "         4.40742254e-01,  6.25335157e-01],\n",
       "       [ 8.53832439e-02, -3.53480577e-01, -7.63740540e-02,\n",
       "         6.10098422e-01,  4.80439931e-01],\n",
       "       [-5.42726517e-02, -7.13103771e-01, -2.66941339e-01,\n",
       "         4.60792959e-01,  5.63645899e-01],\n",
       "       [ 8.71191174e-03, -2.91254044e-01, -1.49555326e-01,\n",
       "         5.98040819e-01,  3.02248657e-01],\n",
       "       [ 9.14165005e-02, -4.63426828e-01, -1.96775839e-01,\n",
       "         6.46002412e-01,  4.62232232e-01],\n",
       "       [-1.14497840e-02, -5.50383449e-01, -9.03076530e-02,\n",
       "         3.58751535e-01,  3.10831845e-01],\n",
       "       [ 1.42154068e-01, -6.20006323e-02, -1.86123520e-01,\n",
       "         1.04199088e+00,  1.95951492e-01],\n",
       "       [-4.91439737e-02, -2.58938432e-01, -3.32842827e-01,\n",
       "         5.69850981e-01,  1.56241328e-01],\n",
       "       [ 1.43006787e-01, -4.16375816e-01,  6.67455494e-02,\n",
       "         6.50679648e-01,  3.60625476e-01],\n",
       "       [ 1.70977816e-01, -4.09780204e-01, -2.80306339e-01,\n",
       "         5.61119616e-01,  4.67536330e-01],\n",
       "       [-7.70080090e-02, -2.42415383e-01, -7.38779604e-02,\n",
       "         8.25079203e-01,  8.87882262e-02],\n",
       "       [ 2.63410509e-01, -3.27155948e-01, -2.34716728e-01,\n",
       "         6.89895391e-01,  1.85696080e-01],\n",
       "       [ 3.82527411e-02, -8.71772647e-01, -1.32342190e-01,\n",
       "         2.52974212e-01,  6.66594684e-01],\n",
       "       [-1.78208619e-01, -2.36241117e-01, -7.60313869e-02,\n",
       "         7.07798421e-01,  3.26312572e-01],\n",
       "       [ 3.06963697e-02, -3.38871837e-01,  3.19122672e-02,\n",
       "         6.49646521e-01,  2.29244530e-01],\n",
       "       [ 1.06650457e-01, -3.21901500e-01, -3.39532852e-01,\n",
       "         6.58181787e-01,  2.53121078e-01],\n",
       "       [-2.62065046e-02, -4.78105605e-01,  5.17364740e-02,\n",
       "         7.30410099e-01,  4.82005656e-01],\n",
       "       [ 8.09192061e-02, -6.31822586e-01, -3.14021528e-01,\n",
       "         5.33613145e-01,  5.92113733e-01],\n",
       "       [ 1.62619084e-01, -1.22889102e-01, -3.77298087e-01,\n",
       "         7.00702429e-01,  2.73074716e-01],\n",
       "       [ 3.03129524e-01, -1.48543984e-01, -1.45718396e-01,\n",
       "         7.20010102e-01,  2.36869842e-01],\n",
       "       [ 5.75114116e-02, -7.50948250e-01,  1.27517074e-01,\n",
       "         6.03170037e-01,  4.92097199e-01],\n",
       "       [ 6.96592331e-02, -6.09230042e-01, -3.21594059e-01,\n",
       "         5.39519787e-01,  4.84691322e-01],\n",
       "       [ 2.68282920e-01, -2.69086629e-01, -3.19999665e-01,\n",
       "         6.73532248e-01,  3.43276262e-01],\n",
       "       [ 2.47141227e-01, -4.57131803e-01,  6.71186149e-02,\n",
       "         4.80832845e-01,  3.77787054e-01],\n",
       "       [ 2.43433625e-01, -1.68553829e-01, -2.28673458e-01,\n",
       "         5.50870299e-01,  2.67005742e-01],\n",
       "       [-4.02843729e-02, -3.75945896e-01, -1.11516193e-01,\n",
       "         7.45006859e-01,  5.31884193e-01],\n",
       "       [ 7.15382248e-02, -5.26004970e-01, -1.55490309e-01,\n",
       "         3.55122060e-01,  5.36253810e-01],\n",
       "       [-2.06068158e-03, -1.60002902e-01,  4.88329232e-02,\n",
       "         7.73450971e-01,  7.75283650e-02],\n",
       "       [ 1.29092738e-01, -6.56859055e-02, -2.80989736e-01,\n",
       "         4.89367247e-01,  3.47950935e-01],\n",
       "       [ 2.54439235e-01, -2.27841720e-01, -1.86154485e-01,\n",
       "         9.35971677e-01,  3.06067348e-01],\n",
       "       [-6.97425604e-02, -1.85775802e-01, -3.45120430e-01,\n",
       "         7.29139090e-01,  4.58362937e-01],\n",
       "       [-3.98927927e-02, -1.43768668e-01, -1.71844810e-01,\n",
       "         9.95614171e-01,  2.51414955e-01],\n",
       "       [ 6.01877272e-02, -6.49439514e-01, -1.43107593e-01,\n",
       "         2.34736323e-01,  4.58104074e-01],\n",
       "       [-5.05853593e-02, -6.79472148e-01, -5.41312099e-02,\n",
       "         2.98509061e-01,  5.49338222e-01],\n",
       "       [ 2.16470987e-01, -2.36882836e-01, -1.49129942e-01,\n",
       "         7.97768712e-01,  3.49787652e-01],\n",
       "       [ 1.13706551e-01, -1.97098061e-01, -1.63132906e-01,\n",
       "         7.68355131e-01,  2.86811233e-01],\n",
       "       [-1.05156600e-01, -2.58382529e-01,  7.84572959e-03,\n",
       "         8.34367514e-01,  1.68383509e-01],\n",
       "       [-2.00790048e-01, -6.23722434e-01, -1.96402371e-01,\n",
       "         2.08852679e-01,  3.96989971e-01],\n",
       "       [-1.46349967e-02, -3.84608030e-01, -2.16415435e-01,\n",
       "         5.67199945e-01,  5.12916207e-01],\n",
       "       [ 1.33978724e-01, -8.28577816e-01, -2.08758503e-01,\n",
       "         1.64914072e-01,  5.57973742e-01],\n",
       "       [ 1.43604577e-01, -4.56911862e-01, -1.88777745e-01,\n",
       "         4.92597014e-01,  2.70999521e-01],\n",
       "       [ 3.83135267e-02, -4.09248739e-01, -3.77766937e-01,\n",
       "         6.48555636e-01,  3.12708616e-01],\n",
       "       [ 1.80456772e-01, -3.42004180e-01, -3.86029631e-01,\n",
       "         6.82002664e-01,  3.91962528e-01],\n",
       "       [ 2.05908760e-01, -2.82799184e-01, -5.09580076e-02,\n",
       "         9.01488900e-01,  4.28092688e-01],\n",
       "       [ 4.38485295e-02, -8.88382375e-01, -1.42735243e-02,\n",
       "         5.20544052e-01,  3.03422481e-01],\n",
       "       [ 1.80031329e-01, -2.24596754e-01, -1.01182371e-01,\n",
       "         3.04117471e-01,  2.48200804e-01],\n",
       "       [ 1.71664268e-01, -3.12762439e-01, -2.05282211e-01,\n",
       "         8.63323689e-01,  3.53462160e-01],\n",
       "       [ 1.71365499e-01, -3.05253029e-01, -3.94219697e-01,\n",
       "         6.41950190e-01,  2.09953114e-01],\n",
       "       [-1.70573249e-01, -6.44690573e-01, -1.11646041e-01,\n",
       "         4.22303230e-01,  5.59684515e-01],\n",
       "       [ 1.49492025e-02, -5.41642547e-01, -1.63615897e-01,\n",
       "         3.07096958e-01,  3.47670406e-01],\n",
       "       [ 2.41684526e-01, -2.50596255e-01, -2.37965032e-01,\n",
       "         7.04684973e-01,  3.06020916e-01]], dtype=float32), label_ids=array([[0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0.],\n",
       "       [1., 1., 0., 1., 0.],\n",
       "       [1., 1., 0., 0., 0.],\n",
       "       [0., 1., 0., 0., 0.],\n",
       "       [0., 0., 0., 1., 0.],\n",
       "       [0., 0., 0., 0., 0.],\n",
       "       [1., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0.],\n",
       "       [1., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 1., 0.],\n",
       "       [1., 0., 0., 1., 1.],\n",
       "       [0., 0., 0., 1., 0.],\n",
       "       [0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0.],\n",
       "       [0., 1., 0., 0., 0.],\n",
       "       [0., 0., 0., 1., 0.],\n",
       "       [0., 0., 0., 0., 0.],\n",
       "       [1., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 1., 0.],\n",
       "       [0., 0., 1., 0., 0.],\n",
       "       [1., 0., 0., 0., 0.],\n",
       "       [1., 0., 0., 1., 0.],\n",
       "       [1., 0., 0., 1., 0.],\n",
       "       [0., 0., 1., 0., 0.],\n",
       "       [1., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0.],\n",
       "       [1., 0., 0., 0., 0.],\n",
       "       [1., 0., 1., 1., 1.],\n",
       "       [0., 1., 0., 1., 0.],\n",
       "       [0., 0., 0., 0., 0.],\n",
       "       [1., 0., 0., 0., 0.],\n",
       "       [1., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 1., 0.],\n",
       "       [1., 0., 1., 0., 0.],\n",
       "       [0., 1., 0., 1., 0.],\n",
       "       [1., 0., 0., 0., 0.],\n",
       "       [1., 0., 0., 1., 0.],\n",
       "       [1., 0., 0., 1., 0.],\n",
       "       [0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 1., 0.],\n",
       "       [0., 0., 0., 0., 0.],\n",
       "       [1., 0., 0., 0., 0.],\n",
       "       [1., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 1., 1.],\n",
       "       [1., 1., 0., 1., 0.],\n",
       "       [1., 0., 0., 1., 1.],\n",
       "       [1., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 1.],\n",
       "       [1., 1., 0., 1., 1.],\n",
       "       [1., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0.],\n",
       "       [0., 0., 1., 0., 0.],\n",
       "       [0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 1.],\n",
       "       [0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 1., 0.],\n",
       "       [0., 0., 0., 1., 0.],\n",
       "       [0., 0., 0., 1., 0.],\n",
       "       [0., 0., 0., 1., 0.],\n",
       "       [0., 0., 0., 1., 0.],\n",
       "       [1., 0., 0., 0., 0.],\n",
       "       [1., 0., 0., 0., 1.],\n",
       "       [0., 0., 0., 1., 0.],\n",
       "       [1., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 1., 0.],\n",
       "       [0., 0., 0., 0., 0.],\n",
       "       [1., 0., 0., 0., 0.],\n",
       "       [0., 1., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0.],\n",
       "       [1., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0.],\n",
       "       [0., 1., 0., 0., 0.],\n",
       "       [1., 0., 0., 0., 0.],\n",
       "       [1., 0., 0., 1., 0.],\n",
       "       [0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 1., 0.],\n",
       "       [0., 0., 0., 1., 1.],\n",
       "       [0., 0., 0., 0., 0.],\n",
       "       [1., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 1., 0.],\n",
       "       [0., 1., 0., 0., 0.],\n",
       "       [1., 0., 0., 0., 0.],\n",
       "       [1., 0., 0., 0., 0.],\n",
       "       [0., 1., 0., 0., 0.],\n",
       "       [1., 0., 0., 0., 0.],\n",
       "       [0., 1., 0., 0., 0.],\n",
       "       [1., 0., 0., 1., 1.],\n",
       "       [1., 1., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0.],\n",
       "       [1., 0., 0., 0., 0.],\n",
       "       [1., 0., 0., 0., 0.],\n",
       "       [1., 0., 0., 0., 0.]], dtype=float32), metrics={'test_loss': 1.474513053894043, 'test_runtime': 59.645, 'test_samples_per_second': 1.677, 'test_steps_per_second': 0.218})"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preds = trainer.predict(val_dataset)\n",
    "preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.5596566 , 0.5017008 , 0.44246215, 0.553499  , 0.5168284 ],\n",
       "       [0.5560532 , 0.5339529 , 0.3784678 , 0.5812366 , 0.4403819 ],\n",
       "       [0.4711132 , 0.5828854 , 0.45413065, 0.49687284, 0.45692253],\n",
       "       [0.5299767 , 0.46306592, 0.42286575, 0.55005836, 0.4875898 ],\n",
       "       [0.4848466 , 0.5505157 , 0.43499762, 0.53591657, 0.43466163],\n",
       "       [0.5593149 , 0.558594  , 0.45497262, 0.58731556, 0.48736408],\n",
       "       [0.56025034, 0.44336146, 0.42529953, 0.5171849 , 0.4468247 ],\n",
       "       [0.5331194 , 0.5653038 , 0.43823856, 0.55371565, 0.4658319 ],\n",
       "       [0.47057995, 0.51654625, 0.40125751, 0.5644671 , 0.49133053],\n",
       "       [0.50247   , 0.64521986, 0.5430379 , 0.54836124, 0.49559474],\n",
       "       [0.5702121 , 0.5672348 , 0.44686088, 0.50926226, 0.45129716],\n",
       "       [0.43598104, 0.55442256, 0.3894622 , 0.559431  , 0.45803875],\n",
       "       [0.5600321 , 0.5168124 , 0.42000622, 0.5372617 , 0.48540455],\n",
       "       [0.47850484, 0.6429855 , 0.4236073 , 0.4823718 , 0.44471532],\n",
       "       [0.4660152 , 0.5744736 , 0.45381486, 0.5205243 , 0.50995   ],\n",
       "       [0.5217763 , 0.56711274, 0.41969943, 0.53530425, 0.48151788],\n",
       "       [0.6109063 , 0.5091593 , 0.42460337, 0.55653733, 0.465503  ],\n",
       "       [0.4944905 , 0.5313058 , 0.37758392, 0.5507741 , 0.4559873 ],\n",
       "       [0.4633094 , 0.60919595, 0.46977317, 0.5702659 , 0.5144733 ],\n",
       "       [0.53880495, 0.62433624, 0.491868  , 0.51099575, 0.5031633 ],\n",
       "       [0.48676986, 0.5920761 , 0.42950532, 0.50826705, 0.48318413],\n",
       "       [0.5548383 , 0.5381357 , 0.42752123, 0.5452655 , 0.51352674],\n",
       "       [0.43015856, 0.5454612 , 0.41434744, 0.52285874, 0.4614206 ],\n",
       "       [0.52427506, 0.465957  , 0.4237246 , 0.5238438 , 0.4882274 ],\n",
       "       [0.5772465 , 0.5458636 , 0.4326869 , 0.492136  , 0.4382707 ],\n",
       "       [0.5227517 , 0.49410895, 0.46194887, 0.5009167 , 0.45319253],\n",
       "       [0.45548654, 0.52842766, 0.39701644, 0.5594052 , 0.4746059 ],\n",
       "       [0.44249547, 0.5629091 , 0.4238249 , 0.55723983, 0.4664182 ],\n",
       "       [0.5374899 , 0.4738623 , 0.39504334, 0.551671  , 0.46443567],\n",
       "       [0.45681235, 0.6203198 , 0.5060901 , 0.5198639 , 0.55480075],\n",
       "       [0.59615076, 0.4253298 , 0.45061678, 0.54407257, 0.44613317],\n",
       "       [0.6049208 , 0.49057278, 0.3976733 , 0.57066905, 0.48793066],\n",
       "       [0.52636373, 0.6063247 , 0.5112056 , 0.51212895, 0.4968051 ],\n",
       "       [0.4619476 , 0.47314516, 0.42465475, 0.5267503 , 0.5035687 ],\n",
       "       [0.60076916, 0.40984946, 0.4203893 , 0.5309255 , 0.43681008],\n",
       "       [0.55678535, 0.5717279 , 0.39834276, 0.5293741 , 0.49893486],\n",
       "       [0.5523836 , 0.58041084, 0.44288582, 0.50162023, 0.47249988],\n",
       "       [0.6417543 , 0.4164755 , 0.41807464, 0.5312199 , 0.47797105],\n",
       "       [0.4985743 , 0.48804674, 0.4113301 , 0.5799847 , 0.46236777],\n",
       "       [0.58293855, 0.5191308 , 0.47026476, 0.51961523, 0.49554807],\n",
       "       [0.51540136, 0.5310722 , 0.4346948 , 0.5574625 , 0.4254983 ],\n",
       "       [0.6054022 , 0.45389915, 0.37186027, 0.6003618 , 0.45033553],\n",
       "       [0.57144326, 0.5351348 , 0.42932808, 0.56240344, 0.4575466 ],\n",
       "       [0.5880747 , 0.40188614, 0.40661862, 0.5622837 , 0.43639767],\n",
       "       [0.53043145, 0.5699522 , 0.47717246, 0.5674413 , 0.50869834],\n",
       "       [0.55999315, 0.5323963 , 0.42400435, 0.50158507, 0.4596665 ],\n",
       "       [0.42986995, 0.6103559 , 0.4717628 , 0.50870484, 0.5153097 ],\n",
       "       [0.4950688 , 0.5867871 , 0.42203453, 0.5652339 , 0.46044946],\n",
       "       [0.5081543 , 0.5914787 , 0.4619714 , 0.4588302 , 0.49157467],\n",
       "       [0.56125194, 0.50298226, 0.4399681 , 0.5680793 , 0.50894254],\n",
       "       [0.5575087 , 0.56217366, 0.42314723, 0.5461205 , 0.4650257 ],\n",
       "       [0.5363914 , 0.52863365, 0.43714118, 0.53359574, 0.4918829 ],\n",
       "       [0.4601383 , 0.5585936 , 0.47973183, 0.49655432, 0.49202275],\n",
       "       [0.52760947, 0.55225134, 0.5213943 , 0.5278611 , 0.50615436],\n",
       "       [0.5997087 , 0.4890708 , 0.45899156, 0.5289374 , 0.49035373],\n",
       "       [0.5804725 , 0.45978075, 0.37588543, 0.5837328 , 0.45732373],\n",
       "       [0.48352513, 0.47980767, 0.44745323, 0.55990875, 0.4562321 ],\n",
       "       [0.44993347, 0.59655905, 0.48029393, 0.529261  , 0.51053333],\n",
       "       [0.5268618 , 0.5469784 , 0.42015254, 0.49294657, 0.46415222],\n",
       "       [0.4844478 , 0.57748044, 0.44826528, 0.50601166, 0.47219816],\n",
       "       [0.50318056, 0.5801902 , 0.41076297, 0.5603495 , 0.44457966],\n",
       "       [0.522449  , 0.56532675, 0.46011618, 0.58487666, 0.5315626 ],\n",
       "       [0.4597013 , 0.6176647 , 0.43618858, 0.53667426, 0.5006873 ],\n",
       "       [0.5286181 , 0.54429954, 0.390465  , 0.554599  , 0.4214634 ],\n",
       "       [0.6062888 , 0.43777135, 0.39242414, 0.5558373 , 0.4564287 ],\n",
       "       [0.5687589 , 0.49925604, 0.39105532, 0.5450982 , 0.47469065],\n",
       "       [0.5396887 , 0.52699447, 0.40382499, 0.560093  , 0.4534557 ],\n",
       "       [0.41886938, 0.5810172 , 0.40339872, 0.55379224, 0.4673774 ],\n",
       "       [0.5048188 , 0.511992  , 0.42821914, 0.5434516 , 0.48780552],\n",
       "       [0.6059385 , 0.48947856, 0.4055516 , 0.5291659 , 0.5184311 ],\n",
       "       [0.45816293, 0.53983337, 0.384671  , 0.5544236 , 0.498076  ],\n",
       "       [0.61276984, 0.4868753 , 0.45278162, 0.53636104, 0.47941533],\n",
       "       [0.4754498 , 0.6268395 , 0.46448714, 0.5605307 , 0.5185044 ],\n",
       "       [0.4857899 , 0.6043344 , 0.4849485 , 0.5328655 , 0.5224661 ],\n",
       "       [0.62005574, 0.48253608, 0.40471652, 0.5171286 , 0.49354154],\n",
       "       [0.52357394, 0.5543327 , 0.48820284, 0.53123707, 0.5942476 ],\n",
       "       [0.44750187, 0.5605831 , 0.4043824 , 0.5431173 , 0.46369463],\n",
       "       [0.63243586, 0.52523184, 0.4288481 , 0.5371391 , 0.48732626],\n",
       "       [0.49716267, 0.5853336 , 0.44695124, 0.51767063, 0.48489502],\n",
       "       [0.5242987 , 0.51532084, 0.43270233, 0.5436694 , 0.42352188],\n",
       "       [0.62290657, 0.48251742, 0.44884905, 0.5513598 , 0.5274144 ],\n",
       "       [0.45331526, 0.63319236, 0.5112087 , 0.53273386, 0.53080636],\n",
       "       [0.6069231 , 0.43163615, 0.33743465, 0.58679175, 0.49672616],\n",
       "       [0.528252  , 0.5016578 , 0.39282992, 0.53987515, 0.46179882],\n",
       "       [0.5855951 , 0.54255015, 0.46987143, 0.52638245, 0.46980762],\n",
       "       [0.4436794 , 0.55591345, 0.40400982, 0.5485394 , 0.47891214],\n",
       "       [0.55524486, 0.47147104, 0.40565127, 0.5728083 , 0.4620584 ],\n",
       "       [0.5142896 , 0.5979406 , 0.42093524, 0.5596572 , 0.48845756],\n",
       "       [0.64068073, 0.5262381 , 0.42126873, 0.47981054, 0.47264588],\n",
       "       [0.6346005 , 0.42950505, 0.3436943 , 0.54835755, 0.43521097],\n",
       "       [0.4456814 , 0.63898355, 0.5036438 , 0.54244405, 0.48367804],\n",
       "       [0.4931474 , 0.5882963 , 0.5122821 , 0.483613  , 0.530226  ],\n",
       "       [0.47434366, 0.5212287 , 0.41320106, 0.56903625, 0.49129325],\n",
       "       [0.505154  , 0.63037366, 0.50603575, 0.5308654 , 0.5220149 ],\n",
       "       [0.6341837 , 0.41319785, 0.35542497, 0.52002984, 0.48719844],\n",
       "       [0.4127817 , 0.6095533 , 0.4651047 , 0.50239   , 0.4721786 ],\n",
       "       [0.5793088 , 0.5077846 , 0.36035886, 0.5517099 , 0.47028652],\n",
       "       [0.43070382, 0.55943173, 0.48097736, 0.59679586, 0.51713544],\n",
       "       [0.43398178, 0.5470085 , 0.4403895 , 0.52176875, 0.47472817],\n",
       "       [0.45409256, 0.6100272 , 0.47564968, 0.5445858 , 0.4984399 ]],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "activation = torch.nn.Sigmoid()\n",
    "activation(torch.tensor(preds.predictions)).numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#cv_df.loc[pd.notna(cv_df['class_weight']),'class_weight'] = ast.literal_eval(cv_df.loc[pd.notna(cv_df['class_weight']),'class_weight'])\n",
    "\n",
    "\n",
    "\n",
    "cv_df.loc[pd.notna(cv_df['class_weight']),'class_weight'] = cv_df.loc[pd.notna(cv_df['class_weight']),'class_weight'].apply(lambda x: ast.literal_eval(x))\n",
    "\n",
    "cv_df.loc[pd.notna(cv_df['class_weight']),'class_weight'] = cv_df.loc[pd.notna(cv_df['class_weight']),'class_weight'].astype(str)\n",
    "best_model = (cv_df[pd.notna(cv_df[scorer])]\n",
    "              .groupby(params)[scorer]\n",
    "              .mean()\n",
    "              .sort_values(ascending=False)\n",
    "              .reset_index()\n",
    "             ).to_dict('records')[0]\n",
    "\n",
    "best_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'1.1.5'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'class_weight': {0: 1, 1: 6.831460674157303}, 'batch_size': 16, 'weight_decay': 0, 'learning_rate': 5e-05, 'num_epochs': 4}\n",
      "{'class_weight': {0: 1, 1: 6.831460674157303}, 'batch_size': 16, 'weight_decay': 0.3, 'learning_rate': 1e-05, 'num_epochs': 2}\n",
      "{'class_weight': {0: 1, 1: 6.831460674157303}, 'batch_size': 16, 'weight_decay': 0.3, 'learning_rate': 1e-05, 'num_epochs': 3}\n",
      "{'class_weight': {0: 1, 1: 6.831460674157303}, 'batch_size': 16, 'weight_decay': 0.3, 'learning_rate': 1e-05, 'num_epochs': 4}\n",
      "{'class_weight': {0: 1, 1: 6.831460674157303}, 'batch_size': 16, 'weight_decay': 0.3, 'learning_rate': 5e-05, 'num_epochs': 2}\n",
      "{'class_weight': {0: 1, 1: 6.831460674157303}, 'batch_size': 16, 'weight_decay': 0.3, 'learning_rate': 5e-05, 'num_epochs': 3}\n",
      "{'class_weight': {0: 1, 1: 6.831460674157303}, 'batch_size': 16, 'weight_decay': 0.3, 'learning_rate': 5e-05, 'num_epochs': 4}\n",
      "{'class_weight': {0: 1, 1: 6.831460674157303}, 'batch_size': 32, 'weight_decay': 0, 'learning_rate': 1e-05, 'num_epochs': 2}\n",
      "{'class_weight': {0: 1, 1: 6.831460674157303}, 'batch_size': 32, 'weight_decay': 0, 'learning_rate': 1e-05, 'num_epochs': 3}\n",
      "{'class_weight': {0: 1, 1: 6.831460674157303}, 'batch_size': 32, 'weight_decay': 0, 'learning_rate': 1e-05, 'num_epochs': 4}\n",
      "{'class_weight': {0: 1, 1: 6.831460674157303}, 'batch_size': 32, 'weight_decay': 0, 'learning_rate': 5e-05, 'num_epochs': 2}\n",
      "{'class_weight': {0: 1, 1: 6.831460674157303}, 'batch_size': 32, 'weight_decay': 0, 'learning_rate': 5e-05, 'num_epochs': 3}\n",
      "{'class_weight': {0: 1, 1: 6.831460674157303}, 'batch_size': 32, 'weight_decay': 0, 'learning_rate': 5e-05, 'num_epochs': 4}\n",
      "{'class_weight': {0: 1, 1: 6.831460674157303}, 'batch_size': 32, 'weight_decay': 0.3, 'learning_rate': 1e-05, 'num_epochs': 2}\n",
      "{'class_weight': {0: 1, 1: 6.831460674157303}, 'batch_size': 32, 'weight_decay': 0.3, 'learning_rate': 1e-05, 'num_epochs': 3}\n",
      "{'class_weight': {0: 1, 1: 6.831460674157303}, 'batch_size': 32, 'weight_decay': 0.3, 'learning_rate': 1e-05, 'num_epochs': 4}\n",
      "{'class_weight': {0: 1, 1: 6.831460674157303}, 'batch_size': 32, 'weight_decay': 0.3, 'learning_rate': 5e-05, 'num_epochs': 2}\n",
      "{'class_weight': {0: 1, 1: 6.831460674157303}, 'batch_size': 32, 'weight_decay': 0.3, 'learning_rate': 5e-05, 'num_epochs': 3}\n",
      "{'class_weight': {0: 1, 1: 6.831460674157303}, 'batch_size': 32, 'weight_decay': 0.3, 'learning_rate': 5e-05, 'num_epochs': 4}\n"
     ]
    }
   ],
   "source": [
    "pr = param_space[0]\n",
    "import ast\n",
    "params_tested=pd.read_csv(fname)[list(pr.keys())].to_dict('records')\n",
    "for pr in params_tested:\n",
    "    if pd.isna(pr[\"class_weight\"]):\n",
    "        pr[\"class_weight\"] = None\n",
    "    if isinstance(pr['class_weight'],str):\n",
    "        pr[\"class_weight\"] = ast.literal_eval(pr[\"class_weight\"])\n",
    "        \n",
    "for pr in param_space:\n",
    "    pr['weight_decay']\n",
    "    if pr in params_tested:\n",
    "        continue\n",
    "    else:\n",
    "        print(pr)\n",
    "        #break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pr['weight_decay'] = 0\n",
    "pr in params_tested"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = {}\n",
    "x[\"weight_decay\"] = 1\n",
    "\n",
    "params = [{\"weight_decay\":1.0},]\n",
    "x in params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'class_weight': {0: 1, 1: 6.831460674157303},\n",
       "  'batch_size': 32,\n",
       "  'weight_decay': 0.0,\n",
       "  'learning_rate': 1e-05,\n",
       "  'num_epochs': 2},\n",
       " {'class_weight': {0: 1, 1: 6.831460674157303},\n",
       "  'batch_size': 32,\n",
       "  'weight_decay': 0.0,\n",
       "  'learning_rate': 1e-05,\n",
       "  'num_epochs': 3},\n",
       " {'class_weight': {0: 1, 1: 6.831460674157303},\n",
       "  'batch_size': 32,\n",
       "  'weight_decay': 0.0,\n",
       "  'learning_rate': 1e-05,\n",
       "  'num_epochs': 4},\n",
       " {'class_weight': {0: 1, 1: 6.831460674157303},\n",
       "  'batch_size': 32,\n",
       "  'weight_decay': 0.0,\n",
       "  'learning_rate': 1e-05,\n",
       "  'num_epochs': 2},\n",
       " {'class_weight': {0: 1, 1: 6.831460674157303},\n",
       "  'batch_size': 32,\n",
       "  'weight_decay': 0.0,\n",
       "  'learning_rate': 1e-05,\n",
       "  'num_epochs': 3},\n",
       " {'class_weight': {0: 1, 1: 6.831460674157303},\n",
       "  'batch_size': 32,\n",
       "  'weight_decay': 0.0,\n",
       "  'learning_rate': 1e-05,\n",
       "  'num_epochs': 4},\n",
       " {'class_weight': {0: 1, 1: 6.831460674157303},\n",
       "  'batch_size': 32,\n",
       "  'weight_decay': 0.0,\n",
       "  'learning_rate': 1e-05,\n",
       "  'num_epochs': 2},\n",
       " {'class_weight': {0: 1, 1: 6.831460674157303},\n",
       "  'batch_size': 32,\n",
       "  'weight_decay': 0.0,\n",
       "  'learning_rate': 1e-05,\n",
       "  'num_epochs': 3},\n",
       " {'class_weight': {0: 1, 1: 6.831460674157303},\n",
       "  'batch_size': 32,\n",
       "  'weight_decay': 0.0,\n",
       "  'learning_rate': 1e-05,\n",
       "  'num_epochs': 4},\n",
       " {'class_weight': {0: 1, 1: 6.831460674157303},\n",
       "  'batch_size': 32,\n",
       "  'weight_decay': 0.0,\n",
       "  'learning_rate': 1e-05,\n",
       "  'num_epochs': 2},\n",
       " {'class_weight': {0: 1, 1: 6.831460674157303},\n",
       "  'batch_size': 32,\n",
       "  'weight_decay': 0.0,\n",
       "  'learning_rate': 1e-05,\n",
       "  'num_epochs': 3},\n",
       " {'class_weight': {0: 1, 1: 6.831460674157303},\n",
       "  'batch_size': 32,\n",
       "  'weight_decay': 0.0,\n",
       "  'learning_rate': 1e-05,\n",
       "  'num_epochs': 4}]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[x for x in params_tested if x['class_weight']!=None and x['batch_size']==32]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_model = (cv_df\n",
    "              .groupby(params)[scorer]\n",
    "              .mean()\n",
    "              .sort_values(ascending=False)\n",
    "              .reset_index()\n",
    "             ).to_dict('records')[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ROC AUC</th>\n",
       "      <th>F1</th>\n",
       "      <th>precision</th>\n",
       "      <th>recall</th>\n",
       "      <th>accuracy</th>\n",
       "      <th>class_weight</th>\n",
       "      <th>batch_size</th>\n",
       "      <th>weight_decay</th>\n",
       "      <th>learning_rate</th>\n",
       "      <th>num_epochs</th>\n",
       "      <th>dataset_size</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>0.808260</td>\n",
       "      <td>0.818692</td>\n",
       "      <td>0.782143</td>\n",
       "      <td>0.858824</td>\n",
       "      <td>0.755051</td>\n",
       "      <td>NaN</td>\n",
       "      <td>16</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00001</td>\n",
       "      <td>2</td>\n",
       "      <td>1960</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.803769</td>\n",
       "      <td>0.813869</td>\n",
       "      <td>0.761092</td>\n",
       "      <td>0.874510</td>\n",
       "      <td>0.742424</td>\n",
       "      <td>NaN</td>\n",
       "      <td>16</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00001</td>\n",
       "      <td>3</td>\n",
       "      <td>1960</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.791893</td>\n",
       "      <td>0.799228</td>\n",
       "      <td>0.787072</td>\n",
       "      <td>0.811765</td>\n",
       "      <td>0.737374</td>\n",
       "      <td>NaN</td>\n",
       "      <td>16</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00001</td>\n",
       "      <td>4</td>\n",
       "      <td>1960</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.807064</td>\n",
       "      <td>0.808587</td>\n",
       "      <td>0.743421</td>\n",
       "      <td>0.886275</td>\n",
       "      <td>0.729798</td>\n",
       "      <td>NaN</td>\n",
       "      <td>16</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00005</td>\n",
       "      <td>2</td>\n",
       "      <td>1960</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.791573</td>\n",
       "      <td>0.802326</td>\n",
       "      <td>0.793103</td>\n",
       "      <td>0.811765</td>\n",
       "      <td>0.742424</td>\n",
       "      <td>NaN</td>\n",
       "      <td>16</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00005</td>\n",
       "      <td>3</td>\n",
       "      <td>1960</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>86</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.356061</td>\n",
       "      <td>{0: 1, 1: 6.831460674157303}</td>\n",
       "      <td>16</td>\n",
       "      <td>0.3</td>\n",
       "      <td>0.00005</td>\n",
       "      <td>3</td>\n",
       "      <td>1960</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>87</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.356061</td>\n",
       "      <td>{0: 1, 1: 6.831460674157303}</td>\n",
       "      <td>16</td>\n",
       "      <td>0.3</td>\n",
       "      <td>0.00005</td>\n",
       "      <td>4</td>\n",
       "      <td>1960</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>88</td>\n",
       "      <td>0.735002</td>\n",
       "      <td>0.788253</td>\n",
       "      <td>0.650510</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.654040</td>\n",
       "      <td>{0: 1, 1: 6.831460674157303}</td>\n",
       "      <td>32</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00001</td>\n",
       "      <td>2</td>\n",
       "      <td>1960</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>89</td>\n",
       "      <td>0.792032</td>\n",
       "      <td>0.791667</td>\n",
       "      <td>0.669377</td>\n",
       "      <td>0.968627</td>\n",
       "      <td>0.671717</td>\n",
       "      <td>{0: 1, 1: 6.831460674157303}</td>\n",
       "      <td>32</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00001</td>\n",
       "      <td>3</td>\n",
       "      <td>1960</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>90</td>\n",
       "      <td>0.788388</td>\n",
       "      <td>0.796148</td>\n",
       "      <td>0.673913</td>\n",
       "      <td>0.972549</td>\n",
       "      <td>0.679293</td>\n",
       "      <td>{0: 1, 1: 6.831460674157303}</td>\n",
       "      <td>32</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00001</td>\n",
       "      <td>4</td>\n",
       "      <td>1960</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>91 rows × 11 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     ROC AUC        F1  precision    recall  accuracy  \\\n",
       "0   0.808260  0.818692   0.782143  0.858824  0.755051   \n",
       "1   0.803769  0.813869   0.761092  0.874510  0.742424   \n",
       "2   0.791893  0.799228   0.787072  0.811765  0.737374   \n",
       "3   0.807064  0.808587   0.743421  0.886275  0.729798   \n",
       "4   0.791573  0.802326   0.793103  0.811765  0.742424   \n",
       "..       ...       ...        ...       ...       ...   \n",
       "86  0.500000  0.000000   0.000000  0.000000  0.356061   \n",
       "87  0.500000  0.000000   0.000000  0.000000  0.356061   \n",
       "88  0.735002  0.788253   0.650510  1.000000  0.654040   \n",
       "89  0.792032  0.791667   0.669377  0.968627  0.671717   \n",
       "90  0.788388  0.796148   0.673913  0.972549  0.679293   \n",
       "\n",
       "                    class_weight  batch_size  weight_decay  learning_rate  \\\n",
       "0                            NaN          16           0.0        0.00001   \n",
       "1                            NaN          16           0.0        0.00001   \n",
       "2                            NaN          16           0.0        0.00001   \n",
       "3                            NaN          16           0.0        0.00005   \n",
       "4                            NaN          16           0.0        0.00005   \n",
       "..                           ...         ...           ...            ...   \n",
       "86  {0: 1, 1: 6.831460674157303}          16           0.3        0.00005   \n",
       "87  {0: 1, 1: 6.831460674157303}          16           0.3        0.00005   \n",
       "88  {0: 1, 1: 6.831460674157303}          32           0.0        0.00001   \n",
       "89  {0: 1, 1: 6.831460674157303}          32           0.0        0.00001   \n",
       "90  {0: 1, 1: 6.831460674157303}          32           0.0        0.00001   \n",
       "\n",
       "    num_epochs  dataset_size  \n",
       "0            2          1960  \n",
       "1            3          1960  \n",
       "2            4          1960  \n",
       "3            2          1960  \n",
       "4            3          1960  \n",
       "..         ...           ...  \n",
       "86           3          1960  \n",
       "87           4          1960  \n",
       "88           2          1960  \n",
       "89           3          1960  \n",
       "90           4          1960  \n",
       "\n",
       "[91 rows x 11 columns]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cv_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "params = list(bert_params.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>class_weight</th>\n",
       "      <th>batch_size</th>\n",
       "      <th>weight_decay</th>\n",
       "      <th>learning_rate</th>\n",
       "      <th>num_epochs</th>\n",
       "      <th>F1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>{0: 1, 1: 6.831460674157303}</td>\n",
       "      <td>16</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00001</td>\n",
       "      <td>4</td>\n",
       "      <td>0.802815</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>{0: 1, 1: 6.831460674157303}</td>\n",
       "      <td>16</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00001</td>\n",
       "      <td>3</td>\n",
       "      <td>0.795534</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>{0: 1, 1: 6.831460674157303}</td>\n",
       "      <td>16</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00005</td>\n",
       "      <td>2</td>\n",
       "      <td>0.794413</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>{0: 1, 1: 6.831460674157303}</td>\n",
       "      <td>32</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00001</td>\n",
       "      <td>4</td>\n",
       "      <td>0.793147</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>{0: 1, 1: 6.831460674157303}</td>\n",
       "      <td>16</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00005</td>\n",
       "      <td>4</td>\n",
       "      <td>0.792043</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>{0: 1, 1: 6.831460674157303}</td>\n",
       "      <td>16</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00005</td>\n",
       "      <td>3</td>\n",
       "      <td>0.791254</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>{0: 1, 1: 6.831460674157303}</td>\n",
       "      <td>32</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00001</td>\n",
       "      <td>3</td>\n",
       "      <td>0.790273</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>{0: 1, 1: 6.831460674157303}</td>\n",
       "      <td>32</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00001</td>\n",
       "      <td>2</td>\n",
       "      <td>0.788962</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>{0: 1, 1: 6.831460674157303}</td>\n",
       "      <td>16</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00001</td>\n",
       "      <td>2</td>\n",
       "      <td>0.788099</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>{0: 1, 1: 6.831460674157303}</td>\n",
       "      <td>16</td>\n",
       "      <td>0.3</td>\n",
       "      <td>0.00005</td>\n",
       "      <td>4</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>{0: 1, 1: 6.831460674157303}</td>\n",
       "      <td>16</td>\n",
       "      <td>0.3</td>\n",
       "      <td>0.00005</td>\n",
       "      <td>3</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11</td>\n",
       "      <td>{0: 1, 1: 6.831460674157303}</td>\n",
       "      <td>16</td>\n",
       "      <td>0.3</td>\n",
       "      <td>0.00005</td>\n",
       "      <td>2</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12</td>\n",
       "      <td>{0: 1, 1: 6.831460674157303}</td>\n",
       "      <td>16</td>\n",
       "      <td>0.3</td>\n",
       "      <td>0.00001</td>\n",
       "      <td>4</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13</td>\n",
       "      <td>{0: 1, 1: 6.831460674157303}</td>\n",
       "      <td>16</td>\n",
       "      <td>0.3</td>\n",
       "      <td>0.00001</td>\n",
       "      <td>3</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14</td>\n",
       "      <td>{0: 1, 1: 6.831460674157303}</td>\n",
       "      <td>16</td>\n",
       "      <td>0.3</td>\n",
       "      <td>0.00001</td>\n",
       "      <td>2</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                    class_weight  batch_size  weight_decay  learning_rate  \\\n",
       "0   {0: 1, 1: 6.831460674157303}          16           0.0        0.00001   \n",
       "1   {0: 1, 1: 6.831460674157303}          16           0.0        0.00001   \n",
       "2   {0: 1, 1: 6.831460674157303}          16           0.0        0.00005   \n",
       "3   {0: 1, 1: 6.831460674157303}          32           0.0        0.00001   \n",
       "4   {0: 1, 1: 6.831460674157303}          16           0.0        0.00005   \n",
       "5   {0: 1, 1: 6.831460674157303}          16           0.0        0.00005   \n",
       "6   {0: 1, 1: 6.831460674157303}          32           0.0        0.00001   \n",
       "7   {0: 1, 1: 6.831460674157303}          32           0.0        0.00001   \n",
       "8   {0: 1, 1: 6.831460674157303}          16           0.0        0.00001   \n",
       "9   {0: 1, 1: 6.831460674157303}          16           0.3        0.00005   \n",
       "10  {0: 1, 1: 6.831460674157303}          16           0.3        0.00005   \n",
       "11  {0: 1, 1: 6.831460674157303}          16           0.3        0.00005   \n",
       "12  {0: 1, 1: 6.831460674157303}          16           0.3        0.00001   \n",
       "13  {0: 1, 1: 6.831460674157303}          16           0.3        0.00001   \n",
       "14  {0: 1, 1: 6.831460674157303}          16           0.3        0.00001   \n",
       "\n",
       "    num_epochs        F1  \n",
       "0            4  0.802815  \n",
       "1            3  0.795534  \n",
       "2            2  0.794413  \n",
       "3            4  0.793147  \n",
       "4            4  0.792043  \n",
       "5            3  0.791254  \n",
       "6            3  0.790273  \n",
       "7            2  0.788962  \n",
       "8            2  0.788099  \n",
       "9            4  0.000000  \n",
       "10           3  0.000000  \n",
       "11           2  0.000000  \n",
       "12           4  0.000000  \n",
       "13           3  0.000000  \n",
       "14           2  0.000000  "
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "best_model = (cv_df\n",
    "              .groupby(params)[scorer]\n",
    "              .mean()\n",
    "              .sort_values(ascending=False)\n",
    "              .reset_index()\n",
    "             )\n",
    "best_model"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
