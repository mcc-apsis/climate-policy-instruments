{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/usr/bin/env python                                                                                                                                                 \n",
    "# Import libraries                                                                                                                                                    \n",
    "import sys\n",
    "import argparse\n",
    "import time\n",
    "import ast\n",
    "import re\n",
    "\n",
    "class args():\n",
    "    pass\n",
    "args.model_name = \"allenai/scibert_scivocab_uncased\"\n",
    "args.model_name = \"distilbert-base-uncased\"\n",
    "args.model_name = \"climatebert__distilroberta-base-climate-f\"\n",
    "#args.model_name = \"svm\"\n",
    "args.y_prefix = \"4 -\"\n",
    "args.n_splits = 3\n",
    "args.make_predictions = \"False\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9\n",
      "Rank I  0 Rank j 3\n",
      "['4 - 1. Economic instruments', '4 - 2. Regulatory Instruments', '4 - 3. Information, education and training', '4 - 4. Governance, strategies and targets', '4 - 5. Agreements']\n",
      "seen_index Int64Index([  0,   1,   2,   3,   4,   5,   6,   7,   8,   9,\n",
      "            ...\n",
      "            793, 794, 795, 796, 797, 798, 799, 800, 801, 802],\n",
      "           dtype='int64', length=803)\n",
      "nonrandom_index Int64Index([  7,  19,  37,  45,  50,  64, 154, 160, 217, 221, 227, 231, 234,\n",
      "            237, 243, 253, 294, 336, 343, 359, 376, 389, 446, 487, 505, 513,\n",
      "            547, 594, 595, 596, 599, 618, 627, 672, 705, 707, 713, 731, 733,\n",
      "            757, 765, 778],\n",
      "           dtype='int64')\n",
      "551 252\n",
      "545 258\n",
      "552 251\n"
     ]
    }
   ],
   "source": [
    "#!/usr/bin/env python\n",
    "# Import libraries\n",
    "import sys\n",
    "import argparse\n",
    "import time\n",
    "import ast\n",
    "\n",
    "class args():\n",
    "    pass\n",
    "args.model_name = \"allenai/scibert_scivocab_uncased\"\n",
    "args.model_name = \"distilbert-base-uncased\"\n",
    "args.model_name = \"climatebert__distilroberta-base-climate-f\"\n",
    "#args.model_name = \"svm\"\n",
    "args.y_prefix = \"4 -\"\n",
    "args.n_splits = 3\n",
    "args.make_predictions = \"False\"\n",
    "\n",
    "t0 = time.time()\n",
    "\n",
    "# Establish what task number we have if running from slurm, otherwise just get a random number\n",
    "# This means we are just running the script in test mode\n",
    "try:\n",
    "    from mpi4py import MPI\n",
    "    comm = MPI.COMM_WORLD\n",
    "    rank = comm.Get_rank()\n",
    "    test = False\n",
    "except:\n",
    "    import random\n",
    "    rank = random.randint(0,args.n_splits**2)\n",
    "    print(rank)\n",
    "    test = True\n",
    "    \n",
    "rank_i = rank%args.n_splits\n",
    "rank_j = rank//args.n_splits\n",
    "\n",
    "print(\"Rank I \", rank_i, \"Rank j\", rank_j)\n",
    "# Import the rest of the libraries\n",
    "import gc\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "from sklearn.model_selection import GridSearchCV, cross_val_score, cross_validate, KFold\n",
    "import pickle\n",
    "import re\n",
    "\n",
    "# Load data\n",
    "seen_df = pd.read_csv('../data/0_labelled_documents.csv')\n",
    "\n",
    "seen_df = (seen_df\n",
    "      .sort_values('id')\n",
    "      .sample(frac=1, random_state=1)\n",
    "      .reset_index(drop=True)\n",
    ")\n",
    "\n",
    "weights_df = pd.read_csv('../data/0_label_weights.csv')\n",
    "\n",
    "# Get the target labels from the y_prefix argument passed to this script\n",
    "if len(args.y_prefix) < 2:\n",
    "    args.y_prefix+=\" \"\n",
    "cols = [x for x in seen_df.columns if re.match(f\"^{args.y_prefix}\",x)]\n",
    "print(cols)\n",
    "num_labels=len(cols)\n",
    "\n",
    "# If the target is inclusion, use only those documents for which we have a non-na value\n",
    "# Otherwise, only use those documents which are included\n",
    "# also define what subset is to be treated as a random representative sample\n",
    "# For labels beyond inclusion, we treat all those that are representative of the included\n",
    "# studies as representative\n",
    "if \"INCLUDE\" in args.y_prefix:\n",
    "    y_var = cols[0]\n",
    "    seen_df = seen_df.loc[pd.notna(seen_df[y_var]),:].reset_index(drop=True)\n",
    "    seen_df['random'] = seen_df['representative_sample']\n",
    "else:\n",
    "    seen_df = seen_df[seen_df['INCLUDE']==1]\n",
    "    seen_df['random'] = seen_df['representative_relevant']\n",
    "    \n",
    "\n",
    "# Turn the columns into target variables and get class-weights to counteract class imbalances\n",
    "if len(cols)==1:\n",
    "    y_var = cols[0]\n",
    "    seen_df = seen_df.loc[pd.notna(seen_df[y_var]),:].reset_index(drop=True)\n",
    "    print(seen_df.shape)\n",
    "    seen_df['labels'] = list(seen_df[y_var].values.astype(int))\n",
    "    cw = seen_df[(seen_df['random']==1) & (seen_df[y_var]==0)].shape[0] / seen_df[(seen_df['random']==1) & (seen_df[y_var]==1)].shape[0]\n",
    "    class_weight={1:cw}\n",
    "    scorer = \"F1\"\n",
    "    weights_df[\"sample_weight\"] = list(weights_df[y_var+\"_sample_weight\"].fillna(1).values)\n",
    "else:\n",
    "    num_labels = len(cols) \n",
    "    weights_df['sample_weight'] = list(weights_df[[x+\"_sample_weight\" for x in cols]].fillna(1).values)\n",
    "    seen_df = seen_df.replace(2,1)\n",
    "    seen_df['labels'] = list(seen_df[cols].values.astype(int))\n",
    "    seen_df = seen_df.dropna(subset=cols)\n",
    "    seen_df = seen_df.reset_index(drop=True)\n",
    "    scorer = \"F1 macro\"\n",
    "    class_weight = {}\n",
    "    for i, t in enumerate(cols):\n",
    "        cw = seen_df[(seen_df['random']==1) & (seen_df[t]==0)].shape[0] / seen_df[(seen_df['random']==1) & (seen_df[t]==1)].shape[0]\n",
    "        class_weight[i] = cw\n",
    "\n",
    "# Remove unneccessary columns\n",
    "seen_df = seen_df[[\"id\",\"title\",\"content\",\"labels\",\"random\"]+cols].merge(\n",
    "    weights_df[[\"doc__id\",\"sample_weight\"]].rename(columns={\"doc__id\":\"id\"})\n",
    ")\n",
    "\n",
    "\n",
    "# Merge with the unseen data if necessary\n",
    "seen_df['seen']  = 1\n",
    "if args.make_predictions==\"True\":\n",
    "    unseen_df = pd.read_csv('../data/0_unlabelled_documents.csv')\n",
    "    unseen_df['seen'] = 0\n",
    "    df = (pd.concat([seen_df,unseen_df])\n",
    "          .sort_values('id')\n",
    "          .sample(frac=1, random_state=1)\n",
    "          .reset_index(drop=True)\n",
    "    )\n",
    "    df.content = df.content.astype(str)\n",
    "else:\n",
    "    df = seen_df\n",
    "\n",
    "# This is the index of nonrandom/nonrepresentative documents, and these will be removed from validation sets\n",
    "nonrandom_index = df[(df['random']!=1) & (df['seen']==1)].index\n",
    "random_index = df[df['random']==1].index\n",
    "seen_index = df[df['seen']==1].index\n",
    "unseen_index = df[df['seen']==0].index\n",
    "\n",
    "print(\"seen_index\", seen_index)\n",
    "print(\"nonrandom_index\", nonrandom_index)\n",
    "\n",
    "# Try this with tensorflow if this is True, otherwise we do it with pytorch\n",
    "try_tf = False\n",
    "if \"distilbert\" in args.model_name.lower() and try_tf:\n",
    "    # Start setting up the Deep learning things\n",
    "    from transformers import BertTokenizer, DistilBertTokenizer,  TFBertForSequenceClassification, TFDistilBertForSequenceClassification\n",
    "    import tensorflow as tf\n",
    "    gpus = tf.config.experimental.list_physical_devices('GPU')\n",
    "    for gpu in gpus:\n",
    "      tf.config.experimental.set_memory_growth(gpu, True)\n",
    "\n",
    "    tf.keras.mixed_precision.experimental.set_policy('mixed_float16')\n",
    "    #tf.config.threading.set_intra_op_parallelism_threads(1)\n",
    "    #tf.config.threading.set_inter_op_parallelism_threads(1)\n",
    "\n",
    "    import tensorflow_addons as tfa\n",
    "    use_tf = True\n",
    "else:\n",
    "    import torch\n",
    "    use_tf = False\n",
    "\n",
    "# import the helper functions defined in cv_setup.py\n",
    "import cv_setup as cvs\n",
    "\n",
    "# Get the BERT parameters, and include class_weight as a parameter to be tested\n",
    "bert_params = cvs.bert_params\n",
    "bert_params['class_weight'].append(class_weight)\n",
    "bert_params['class_weight'] = [class_weight]\n",
    "param_space = list(cvs.product_dict(**bert_params))\n",
    "params = list(bert_params.keys())\n",
    "\n",
    "if test:\n",
    "    param_space = param_space[:2]\n",
    "\n",
    "# Get the spilts for our outer fold, discard=False means to pass the nonrandom documents that\n",
    "# would ordinarily be in the validation set back into the test set\n",
    "outer_cv = cvs.KFoldRandom(args.n_splits, seen_index, nonrandom_index, discard=False)\n",
    "\n",
    "# Iterate through the folds\n",
    "for k, (train, test) in enumerate(outer_cv):    \n",
    "    print(len(train), len(test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['4 - 1. Economic instruments', '4 - 2. Regulatory Instruments', '4 - 3. Information, education and training', '4 - 4. Governance, strategies and targets', '4 - 5. Agreements']\n"
     ]
    }
   ],
   "source": [
    "# Get the target labels from the y_prefix argument passed to this script                                                                                              \n",
    "if len(args.y_prefix) < 2:\n",
    "    args.y_prefix+=\" \"\n",
    "cols = [x for x in seen_df.columns if re.match(f\"^{args.y_prefix}\",x)]\n",
    "print(cols)\n",
    "num_labels=len(cols)\n",
    "\n",
    "# If the target is inclusion, use only those documents for which we have a non-na value                                                                               \n",
    "# Otherwise, only use those documents which are included                                                                                                              \n",
    "# also define what subset is to be treated as a random representative sample                                                                                          \n",
    "# For labels beyond inclusion, we treat all those that are representative of the included                                                                             \n",
    "# studies as representative                                                                                                                                           \n",
    "if \"INCLUDE\" in args.y_prefix:\n",
    "    y_var = cols[0]\n",
    "    seen_df = seen_df.loc[pd.notna(seen_df[y_var]),:].reset_index(drop=True)\n",
    "    seen_df['random'] = seen_df['representative_sample']\n",
    "else:\n",
    "    seen_df = seen_df[seen_df['INCLUDE']==1]\n",
    "    seen_df['random'] = seen_df['representative_relevant']\n",
    "\n",
    "\n",
    "# Turn the columns into target variables and get class-weights to counteract class imbalances                                                                         \n",
    "if len(cols)==1:\n",
    "    y_var = cols[0]\n",
    "    seen_df = seen_df.loc[pd.notna(seen_df[y_var]),:].reset_index(drop=True)\n",
    "    print(seen_df.shape)\n",
    "    seen_df['labels'] = list(seen_df[y_var].values.astype(int))\n",
    "    cw = seen_df[(seen_df['random']==1) & (seen_df[y_var]==0)].shape[0] / seen_df[(seen_df['random']==1) & (seen_df[y_var]==1)].shape[0]\n",
    "    class_weight={1:cw}\n",
    "    scorer = \"F1\"\n",
    "    weights_df[\"sample_weight\"] = list(weights_df[y_var+\"_sample_weight\"].fillna(1).values)\n",
    "else:\n",
    "    num_labels = len(cols)\n",
    "    weights_df['sample_weight'] = list(weights_df[[x+\"_sample_weight\" for x in cols]].fillna(1).values)\n",
    "    seen_df = seen_df.replace(2,1)\n",
    "    seen_df['labels'] = list(seen_df[cols].values.astype(int))\n",
    "    seen_df = seen_df.dropna(subset=cols)\n",
    "    seen_df = seen_df.reset_index(drop=True)\n",
    "    scorer = \"F1 macro\"\n",
    "    class_weight = {}\n",
    "    for i, t in enumerate(cols):\n",
    "        cw = seen_df[(seen_df['random']==1) & (seen_df[t]==0)].shape[0] / seen_df[(seen_df['random']==1) & (seen_df[t]==1)].shape[0]\n",
    "        class_weight[i] = cw\n",
    "\n",
    "# Remove unneccessary columns                                                                                                                                         \n",
    "seen_df = seen_df[[\"id\",\"title\",\"content\",\"labels\",\"random\"]+cols].merge(\n",
    "    weights_df[[\"doc__id\",\"sample_weight\"]].rename(columns={\"doc__id\":\"id\"})\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Merge with the unseen data if necessary                                                                                                                             \n",
    "seen_df['seen']  = 1\n",
    "if args.make_predictions==\"True\":\n",
    "    unseen_df = pd.read_csv('../data/0_unlabelled_documents.csv')\n",
    "    unseen_df['seen'] = 0\n",
    "    df = (pd.concat([seen_df,unseen_df])\n",
    "          .sort_values('id')\n",
    "          .sample(frac=1, random_state=1)\n",
    "          .reset_index(drop=True)\n",
    "    )\n",
    "    df.content = df.content.astype(str)\n",
    "else:\n",
    "    df = seen_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "253\n",
      "1\n",
      "258\n",
      "2\n",
      "249\n"
     ]
    }
   ],
   "source": [
    "# This is the index of nonrandom/nonrepresentative documents, and these will be removed from validation sets                                                          \n",
    "nonrandom_index = df[(df['random']!=1) & (df['seen']==1)].index\n",
    "random_index = df[df['random']==1].index\n",
    "seen_index = df[df['seen']==1].index\n",
    "unseen_index = df[df['seen']==0].index\n",
    "\n",
    "# Try this with tensorflow if this is True, otherwise we do it with pytorch                                                                                           \n",
    "try_tf = False\n",
    "if \"distilbert\" in args.model_name.lower() and try_tf:\n",
    "    # Start setting up the Deep learning things                                                                                                                       \n",
    "    from transformers import BertTokenizer, DistilBertTokenizer,  TFBertForSequenceClassification, TFDistilBertForSequenceClassification\n",
    "    import tensorflow as tf\n",
    "    gpus = tf.config.experimental.list_physical_devices('GPU')\n",
    "    for gpu in gpus:\n",
    "      tf.config.experimental.set_memory_growth(gpu, True)\n",
    "\n",
    "    tf.keras.mixed_precision.experimental.set_policy('mixed_float16')\n",
    "    #tf.config.threading.set_intra_op_parallelism_threads(1)                                                                                                          \n",
    "    #tf.config.threading.set_inter_op_parallelism_threads(1)                                                                                                          \n",
    "\n",
    "    import tensorflow_addons as tfa\n",
    "    use_tf = True\n",
    "else:\n",
    "    import torch\n",
    "    use_tf = False\n",
    "\n",
    "# import the helper functions defined in cv_setup.py                                                                                                                  \n",
    "import cv_setup as cvs\n",
    "\n",
    "# Get the BERT parameters, and include class_weight as a parameter to be tested                                                                                       \n",
    "bert_params = cvs.bert_params\n",
    "bert_params['class_weight'].append(class_weight)\n",
    "bert_params['class_weight'] = [class_weight]\n",
    "param_space = list(cvs.product_dict(**bert_params))\n",
    "params = list(bert_params.keys())\n",
    "\n",
    "def KFoldRandom(n_splits, X, no_test, shuffle=False, discard=True):\n",
    "    kf = KFold(n_splits=n_splits, shuffle=shuffle)\n",
    "    for train, test in kf.split(X):\n",
    "        X = np.array(X)\n",
    "        train = X[train]\n",
    "        test = X[test]\n",
    "        if not discard:\n",
    "            train = list(train) +  [x for x in test if x in no_test]\n",
    "        test = [x for x in test if x not in no_test]\n",
    "        yield (train, test)\n",
    "\n",
    "\n",
    "outer_cv = KFoldRandom(args.n_splits, seen_index, nonrandom_index, discard=False)\n",
    "\n",
    "# Iterate through the folds                                                                                                                                           \n",
    "for k, (train, test) in enumerate(outer_cv):\n",
    "    print(k)\n",
    "    print(len(test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "253\n",
      "0\n",
      "258\n",
      "0\n",
      "249\n",
      "0\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "760"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "outer_cv = KFoldRandom(args.n_splits, seen_index, nonrandom_index, discard=False)\n",
    "\n",
    "total = 0\n",
    "# Iterate through the folds                                                                                                                                           \n",
    "for k, (train, test) in enumerate(outer_cv):\n",
    "    print(len(test))\n",
    "    total+=len(test)\n",
    "    print(len(set(train) & set(test)))\n",
    "    \n",
    "total"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python3.9 (py39)",
   "language": "python",
   "name": "py39"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
