# Import libraries
import sys
import argparse

parser = argparse.ArgumentParser(description="Run the inner loop of a nested cross validation process")
parser.add_argument("n_splits", type=int)
parser.add_argument("model_name", type=str)
parser.add_argument("data_path", type=str)
parser.add_argument("y_prefix", type=str)
parser.add_argument("resume", type=str)

args = parser.parse_args()

# Establish what task number we have if running from slurm, otherwise just get a random number
try:
    from mpi4py import MPI
    comm = MPI.COMM_WORLD
    rank = comm.Get_rank()
    test = False
except:
    import random
    rank = random.randint(0,args.n_splits**2)
    print(rank)
    test = True
    
rank_i = rank//args.n_splits
rank_j = rank%args.n_splits

# Import the rest of the libraries
import gc
import pandas as pd
import numpy as np
import os
from sklearn.model_selection import GridSearchCV, cross_val_score, cross_validate, KFold
import pickle
import re

# Load data
df = pd.read_csv('../data/0_labelled_documents.csv')

df = (df
      .sort_values('id')
      .sample(frac=1, random_state=1)
      .reset_index(drop=True)
)

if test:
    df = df.head(100)


cols = [x for x in df.columns if re.match(f"^{args.y_prefix} ",x)]
print(cols)
num_labels=len(cols)
if len(cols)==1:
    y = df[cols[0]]
    random_index = df[df['representative_sample==1']].index
else:
    random_index = df[df['representative_relevant==1']].index
    
# Start setting up the Deep learning things
from transformers import BertTokenizer, DistilBertTokenizer, TFDistilBertForSequenceClassification, TFBertForSequenceClassification
import tensorflow as tf
import tensorflow_addons as tfa
import cv_setup as cvs

tf.config.threading.set_intra_op_parallelism_threads(8)
tf.config.threading.set_inter_op_parallelism_threads(8)

if "distilbert" in args.model_name:
    tokenizer = DistilBertTokenizer.from_pretrained(args.model_name)
    model = TFDistilBertForSequenceClassification.from_pretrained(args.model_name, num_labels=num_labels)
else:
    tokenizer = BertTokenizer.from_pretrained(args.model_name)
    model = TFBertForSequenceClassification.from_pretrained(args.model_name, num_labels=num_labels)

cw = df[(df['random_sample']==1) & (df['relevant']==0)].shape[0] / df[(df['random_sample']==1) & (df['relevant']==1)].shape[0]
class_weight={0:1, 1:cw}

cvs.bert_params['class_weight'].append(class_weight)


for k, (train, test) in enumerate(outer_cv):    
    if k!=rank_i:
        continue
    inner_cv = KFoldRandom(5, train, df[df['random_sample']!=1].index, discard=False)
    inner_scores = []
    for l, (l_train, l_test) in enumerate(inner_cv):
        if l!=rank_j:
            continue
        fname = f'cv/cv_results_{args.y_prefix}_{args.model_name}_{rank_i}_{rank_j}.csv'
        if args.resume=="True":
            try:
                pr = param_space[0]
                cv_results=pd.read_csv(fname).to_dict('records')
                params_tested=pd.read_csv(fname)[list(pr.keys())].to_dict('records')
            except:
                cv_results = []
                params_tested = []
        else:
            cv_results = []
            params_tested = []
            
        for pr in param_space:
            if pr in params_tested:
                continue
            cv_results.append(train_eval_bert(pr, df=df, train=l_train, test=l_test))
            pd.DataFrame.from_dict(cv_results).to_csv(fname,index=False)
            gc.collect()
